{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8526d48e",
   "metadata": {},
   "source": [
    "# 03b — Chunkr Semantic Enhancement\n",
    "\n",
    "**Pipeline Position:** 01_blocks (OCR) → 02_cleanup (Domain) → 03_llm_cleanup (LLM) → **03b_chunkr_enhance** → 04_json_extraction\n",
    "\n",
    "This stage takes cleaned OCR blocks from the previous pipeline stages and enhances them with:\n",
    "- **Advanced layout analysis** and structure detection\n",
    "- **Semantic chunking** optimized for RAG/LLM applications  \n",
    "- **Cross-reference resolution** and text flow reconstruction\n",
    "- **Enhanced metadata** extraction\n",
    "\n",
    "## How it works:\n",
    "1. Takes cleaned blocks from `03_llm_cleanup`\n",
    "2. Reconstructs page content from OCR blocks\n",
    "3. Sends consolidated content to local Chunkr instance\n",
    "4. Receives back semantically chunked and structured content\n",
    "5. Merges Chunkr insights with original OCR block metadata\n",
    "6. Outputs enhanced blocks ready for JSON extraction\n",
    "\n",
    "## Prerequisites:\n",
    "- Local Chunkr instance running (use `./setup_local_chunkr.sh`)\n",
    "- Completed previous pipeline stages (01, 02, 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "blocks_dir = \"outputs/run_001/03_llmcleaned\"  # Input from LLM cleanup stage\n",
    "output_dir = \"outputs/run_001/03b_chunkr_enhanced\"  # Output for next stage\n",
    "chunkr_base_url = \"http://localhost:8000\"  # Local Chunkr instance\n",
    "\n",
    "# Chunkr processing parameters\n",
    "chunk_target_length = 512\n",
    "chunk_overlap = 50\n",
    "ocr_strategy = \"Auto\"  # Since we already have OCR, this is mainly for layout analysis\n",
    "\n",
    "# Processing limits\n",
    "max_retries = 3\n",
    "timeout_seconds = 300\n",
    "chunkr_wait_seconds = 2\n",
    "\n",
    "print(f\"📁 Input: {blocks_dir}\")\n",
    "print(f\"📁 Output: {output_dir}\")\n",
    "print(f\"🔗 Chunkr: {chunkr_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup directories\n",
    "blocks_dir = Path(blocks_dir).expanduser().resolve()\n",
    "output_dir = Path(output_dir).expanduser().resolve()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Input directory: {blocks_dir}\")\n",
    "print(f\"✅ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Check - Verify Chunkr is available\n",
    "def check_chunkr_health() -> bool:\n",
    "    \"\"\"Check if Chunkr service is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{chunkr_base_url}/health\", timeout=10)\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Chunkr health check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test Chunkr availability\n",
    "if check_chunkr_health():\n",
    "    print(f\"✅ Chunkr service is healthy at {chunkr_base_url}\")\n",
    "    \n",
    "    # Get additional info\n",
    "    try:\n",
    "        response = requests.get(f\"{chunkr_base_url}/llm/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(f\"🤖 Available models: {models.get('models', [])}\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(f\"❌ Chunkr service not available at {chunkr_base_url}\")\n",
    "    print(\"💡 Please run: ./setup_local_chunkr.sh\")\n",
    "    print(\"💡 Or check if Docker services are running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ecead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover input files from previous pipeline stage\n",
    "def discover_input_files(blocks_dir: Path) -> List[Path]:\n",
    "    \"\"\"Discover input JSON files from previous pipeline stage\"\"\"\n",
    "    \n",
    "    # Look for files in order of preference\n",
    "    patterns = [\n",
    "        \"page_*_blocks.llmcleaned.json\",  # From 03_llm_cleanup\n",
    "        \"page_*_blocks.cleaned.json\",     # From 02_cleanup  \n",
    "        \"page_*_blocks.domain.json\",      # From 02_cleanup (alt)\n",
    "        \"page_*_blocks.json\"              # From 01_blocks (fallback)\n",
    "    ]\n",
    "    \n",
    "    files = []\n",
    "    for pattern in patterns:\n",
    "        found = sorted(blocks_dir.glob(pattern))\n",
    "        if found:\n",
    "            files = found\n",
    "            print(f\"📄 Found {len(files)} files with pattern: {pattern}\")\n",
    "            break\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"❌ No compatible block files found in {blocks_dir}\")\n",
    "        print(\"💡 Available files:\")\n",
    "        for f in sorted(blocks_dir.glob(\"*.json\")):\n",
    "            print(f\"   {f.name}\")\n",
    "        return []\n",
    "    \n",
    "    return files\n",
    "\n",
    "input_files = discover_input_files(blocks_dir)\n",
    "print(f\"\\n📊 Processing {len(input_files)} files\")\n",
    "\n",
    "# Show first few files\n",
    "for i, file_path in enumerate(input_files[:5]):\n",
    "    print(f\"   {i+1}. {file_path.name}\")\n",
    "if len(input_files) > 5:\n",
    "    print(f\"   ... and {len(input_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef007a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for content processing\n",
    "def load_page_blocks(file_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load blocks from a page JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def reconstruct_page_content(blocks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Reconstruct page content from blocks for Chunkr processing\"\"\"\n",
    "    \n",
    "    # Sort blocks by position (top to bottom, left to right)\n",
    "    def sort_key(block):\n",
    "        bbox = block.get('bbox', [0, 0, 0, 0])\n",
    "        return (bbox[1], bbox[0])  # y-coordinate first, then x-coordinate\n",
    "    \n",
    "    sorted_blocks = sorted(blocks, key=sort_key)\n",
    "    \n",
    "    # Extract text with structure preservation\n",
    "    lines = []\n",
    "    current_line_y = None\n",
    "    current_line_parts = []\n",
    "    \n",
    "    for block in sorted_blocks:\n",
    "        # Get the best available text (prioritize LLM cleaned)\n",
    "        text = (\n",
    "            block.get('text_final', '') or \n",
    "            block.get('text_llm', '') or \n",
    "            block.get('text', '')\n",
    "        ).strip()\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        bbox = block.get('bbox', [0, 0, 0, 0])\n",
    "        block_y = bbox[1] if len(bbox) > 1 else 0\n",
    "        \n",
    "        # If this block is on a significantly different line, start new line\n",
    "        if current_line_y is None or abs(block_y - current_line_y) > 10:\n",
    "            if current_line_parts:\n",
    "                lines.append(' '.join(current_line_parts))\n",
    "            current_line_parts = [text]\n",
    "            current_line_y = block_y\n",
    "        else:\n",
    "            current_line_parts.append(text)\n",
    "    \n",
    "    # Add the last line\n",
    "    if current_line_parts:\n",
    "        lines.append(' '.join(current_line_parts))\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test content reconstruction with first file\n",
    "if input_files:\n",
    "    test_file = input_files[0]\n",
    "    print(f\"🧪 Testing content reconstruction with: {test_file.name}\")\n",
    "    \n",
    "    page_data = load_page_blocks(test_file)\n",
    "    original_blocks = page_data.get('blocks', [])\n",
    "    \n",
    "    print(f\"📄 Original blocks: {len(original_blocks)}\")\n",
    "    \n",
    "    if original_blocks:\n",
    "        # Show first few blocks\n",
    "        print(\"\\n📝 Sample blocks:\")\n",
    "        for i, block in enumerate(original_blocks[:3]):\n",
    "            text = (\n",
    "                block.get('text_final', '') or \n",
    "                block.get('text_llm', '') or \n",
    "                block.get('text', '')\n",
    "            ).strip()\n",
    "            bbox = block.get('bbox', [])\n",
    "            confidence = block.get('confidence', 0)\n",
    "            print(f\"   {i+1}. '{text[:60]}{'...' if len(text) > 60 else ''}' (conf: {confidence:.2f})\")\n",
    "        \n",
    "        # Reconstruct content\n",
    "        reconstructed = reconstruct_page_content(original_blocks)\n",
    "        print(f\"\\n📝 Reconstructed content ({len(reconstructed)} chars):\")\n",
    "        print(f\"   {reconstructed[:200]}{'...' if len(reconstructed) > 200 else ''}\")\n",
    "    else:\n",
    "        print(\"❌ No blocks found in test file\")\n",
    "else:\n",
    "    print(\"❌ No input files to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98047232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkr processing functions\n",
    "def create_temp_document(content: str, page_num: int) -> str:\n",
    "    \"\"\"Create a temporary text document for Chunkr processing\"\"\"\n",
    "    \n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        mode='w', \n",
    "        suffix=f'_page_{page_num}.txt', \n",
    "        delete=False,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    temp_file.write(content)\n",
    "    temp_file.close()\n",
    "    \n",
    "    return temp_file.name\n",
    "\n",
    "def process_with_chunkr(content: str, page_num: int) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Send content to Chunkr for semantic processing\"\"\"\n",
    "    \n",
    "    if not content.strip():\n",
    "        print(f\"[WARN] Empty content for page {page_num}\")\n",
    "        return None\n",
    "    \n",
    "    temp_file_path = None\n",
    "    try:\n",
    "        # Create temporary document\n",
    "        temp_file_path = create_temp_document(content, page_num)\n",
    "        \n",
    "        # Prepare Chunkr request\n",
    "        with open(temp_file_path, 'rb') as f:\n",
    "            files = {'file': (f'page_{page_num}.txt', f, 'text/plain')}\n",
    "            \n",
    "            data = {\n",
    "                'ocr_strategy': ocr_strategy,\n",
    "                'config': json.dumps({\n",
    "                    \"chunk_processing\": {\n",
    "                        \"target_length\": chunk_target_length,\n",
    "                        \"overlap\": chunk_overlap\n",
    "                    },\n",
    "                    \"segmentation_strategy\": \"LayoutAnalysis\"\n",
    "                })\n",
    "            }\n",
    "            \n",
    "            # Upload to Chunkr\n",
    "            print(f\"   🔄 Uploading to Chunkr...\")\n",
    "            response = requests.post(\n",
    "                f\"{chunkr_base_url}/api/v1/task\",\n",
    "                files=files,\n",
    "                data=data,\n",
    "                timeout=timeout_seconds\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"   ❌ Upload failed: {response.status_code} - {response.text[:200]}\")\n",
    "                return None\n",
    "            \n",
    "            task_data = response.json()\n",
    "            task_id = task_data.get('task_id')\n",
    "            \n",
    "            if not task_id:\n",
    "                print(f\"   ❌ No task_id received\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"   📋 Task ID: {task_id}\")\n",
    "            \n",
    "            # Poll for completion\n",
    "            for attempt in range(150):  # 5 minutes max\n",
    "                task_response = requests.get(\n",
    "                    f\"{chunkr_base_url}/api/v1/task/{task_id}\",\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if task_response.status_code == 200:\n",
    "                    task_status = task_response.json()\n",
    "                    status = task_status.get('status', '').lower()\n",
    "                    \n",
    "                    if status == 'succeeded':\n",
    "                        print(f\"   ✅ Processing completed\")\n",
    "                        return task_status\n",
    "                    elif status in ['failed', 'cancelled']:\n",
    "                        error_msg = task_status.get('message', 'Unknown error')\n",
    "                        print(f\"   ❌ Task failed: {error_msg}\")\n",
    "                        return None\n",
    "                    else:\n",
    "                        if attempt % 10 == 0:  # Print status every 10 attempts\n",
    "                            print(f\"   ⏳ Status: {status}\")\n",
    "                        time.sleep(chunkr_wait_seconds)\n",
    "                else:\n",
    "                    print(f\"   ❌ Failed to check status: {task_response.status_code}\")\n",
    "                    return None\n",
    "            \n",
    "            print(f\"   ⏰ Processing timed out\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Exception: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        if temp_file_path and os.path.exists(temp_file_path):\n",
    "            try:\n",
    "                os.unlink(temp_file_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(\"✅ Chunkr processing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Chunkr processing with sample content\n",
    "if input_files and check_chunkr_health():\n",
    "    print(\"🧪 Testing Chunkr processing...\")\n",
    "    \n",
    "    # Use the reconstructed content from earlier\n",
    "    if 'reconstructed' in locals() and reconstructed:\n",
    "        print(f\"📤 Testing with {len(reconstructed)} characters of content\")\n",
    "        \n",
    "        # Process with Chunkr\n",
    "        test_result = process_with_chunkr(reconstructed, 999)  # Use page 999 for test\n",
    "        \n",
    "        if test_result:\n",
    "            output = test_result.get('output', {})\n",
    "            segments = output.get('segments', [])\n",
    "            \n",
    "            print(f\"\\n✅ Chunkr test successful!\")\n",
    "            print(f\"   📊 Generated {len(segments)} semantic segments\")\n",
    "            print(f\"   📄 File type: {output.get('file_type', 'unknown')}\")\n",
    "            print(f\"   ⏱️ Processing time: {output.get('processing_time_s', 'unknown')}s\")\n",
    "            \n",
    "            # Show sample segments\n",
    "            print(f\"\\n📝 Sample segments:\")\n",
    "            for i, segment in enumerate(segments[:3]):\n",
    "                content = segment.get('content', '').strip()\n",
    "                seg_type = segment.get('segment_type', 'text')\n",
    "                print(f\"   {i+1}. [{seg_type}] {content[:80]}{'...' if len(content) > 80 else ''}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"❌ Chunkr test failed\")\n",
    "    else:\n",
    "        print(\"❌ No content available for testing\")\n",
    "else:\n",
    "    print(\"⏭️ Skipping Chunkr test (no files or service unavailable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result merging function\n",
    "def merge_chunkr_results(original_blocks: List[Dict[str, Any]], \n",
    "                        chunkr_result: Dict[str, Any],\n",
    "                        page_num: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Merge Chunkr semantic analysis with original OCR blocks\"\"\"\n",
    "    \n",
    "    if not chunkr_result:\n",
    "        # If Chunkr failed, return original blocks with semantic metadata\n",
    "        for block in original_blocks:\n",
    "            block['chunkr_processed'] = False\n",
    "            block['semantic_chunk_id'] = None\n",
    "            block['semantic_type'] = 'text'\n",
    "            block['processing_stage'] = '03b_chunkr_enhanced'\n",
    "        return original_blocks\n",
    "    \n",
    "    # Extract Chunkr segments\n",
    "    output = chunkr_result.get('output', {})\n",
    "    segments = output.get('segments', [])\n",
    "    \n",
    "    enhanced_blocks = []\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # Create enhanced block combining Chunkr insights with original structure\n",
    "        enhanced_block = {\n",
    "            # Chunkr semantic content\n",
    "            'text': segment.get('content', '').strip() or segment.get('text', '').strip(),\n",
    "            'text_chunkr': segment.get('content', '').strip(),\n",
    "            'text_final': segment.get('content', '').strip(),  # Use Chunkr as final\n",
    "            \n",
    "            # Semantic metadata\n",
    "            'semantic_chunk_id': f\"page_{page_num}_chunk_{i}\",\n",
    "            'semantic_type': segment.get('segment_type', 'text'),\n",
    "            'chunkr_processed': True,\n",
    "            \n",
    "            # Preserve structure info if available\n",
    "            'bbox': segment.get('bbox', [0, 0, 1, 1]),\n",
    "            'confidence': segment.get('confidence', 0.95),\n",
    "            \n",
    "            # Processing metadata\n",
    "            'chunkr_segment_id': segment.get('segment_id', f'seg_{i}'),\n",
    "            'processing_stage': '03b_chunkr_enhanced',\n",
    "            'enhanced_timestamp': time.time(),\n",
    "            \n",
    "            # Chunkr task metadata\n",
    "            'chunkr_metadata': {\n",
    "                'task_id': chunkr_result.get('task_id'),\n",
    "                'file_type': output.get('file_type', 'text'),\n",
    "                'total_segments': len(segments),\n",
    "                'processing_time_s': output.get('processing_time_s'),\n",
    "                'page_count': output.get('page_count', 1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        enhanced_blocks.append(enhanced_block)\n",
    "    \n",
    "    return enhanced_blocks\n",
    "\n",
    "print(\"✅ Result merging function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop\n",
    "if not input_files:\n",
    "    print(\"❌ No input files found. Please check the input directory.\")\n",
    "elif not check_chunkr_health():\n",
    "    print(\"❌ Chunkr service not available. Please run: ./setup_local_chunkr.sh\")\n",
    "else:\n",
    "    print(f\"🚀 Starting batch processing of {len(input_files)} files...\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, file_path in enumerate(input_files):\n",
    "        print(f\"\\n📄 [{i+1}/{len(input_files)}] Processing: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load original blocks\n",
    "            page_data = load_page_blocks(file_path)\n",
    "            if not page_data:\n",
    "                print(f\"   ❌ Failed to load page data\")\n",
    "                failed += 1\n",
    "                continue\n",
    "            \n",
    "            original_blocks = page_data.get('blocks', [])\n",
    "            if not original_blocks:\n",
    "                print(f\"   ⚠️ No blocks found\")\n",
    "                failed += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"   📊 Loaded {len(original_blocks)} original blocks\")\n",
    "            \n",
    "            # Extract page number\n",
    "            page_num = 0\n",
    "            try:\n",
    "                match = re.search(r'page_(\\d+)', file_path.name)\n",
    "                if match:\n",
    "                    page_num = int(match.group(1))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Reconstruct page content\n",
    "            page_content = reconstruct_page_content(original_blocks)\n",
    "            if not page_content.strip():\n",
    "                print(f\"   ⚠️ No content reconstructed\")\n",
    "                failed += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"   📝 Reconstructed {len(page_content)} characters\")\n",
    "            \n",
    "            # Process with Chunkr\n",
    "            chunkr_result = process_with_chunkr(page_content, page_num)\n",
    "            \n",
    "            # Merge results\n",
    "            enhanced_blocks = merge_chunkr_results(original_blocks, chunkr_result, page_num)\n",
    "            \n",
    "            print(f\"   📦 Generated {len(enhanced_blocks)} enhanced blocks\")\n",
    "            \n",
    "            # Create output data structure\n",
    "            output_data = {\n",
    "                'page_num': page_num,\n",
    "                'original_file': str(file_path),\n",
    "                'processing_stage': '03b_chunkr_enhanced',\n",
    "                'timestamp': time.time(),\n",
    "                'chunkr_success': chunkr_result is not None,\n",
    "                'original_blocks_count': len(original_blocks),\n",
    "                'enhanced_blocks_count': len(enhanced_blocks),\n",
    "                'blocks': enhanced_blocks,\n",
    "                'metadata': {\n",
    "                    'chunkr_base_url': chunkr_base_url,\n",
    "                    'chunk_target_length': chunk_target_length,\n",
    "                    'chunk_overlap': chunk_overlap,\n",
    "                    'ocr_strategy': ocr_strategy\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Preserve original metadata\n",
    "            for key in ['page_info', 'image_path', 'source_pdf']:\n",
    "                if key in page_data:\n",
    "                    output_data[key] = page_data[key]\n",
    "            \n",
    "            # Save enhanced blocks\n",
    "            output_file = output_dir / f\"{file_path.stem}.chunkr.json\"\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"   ✅ Saved: {output_file.name}\")\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Exception: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"📊 Processing Summary:\")\n",
    "    print(f\"   ✅ Successful: {successful}\")\n",
    "    print(f\"   ❌ Failed: {failed}\")\n",
    "    print(f\"   📄 Total: {successful + failed}\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        print(f\"\\n📁 Enhanced blocks saved to: {output_dir}\")\n",
    "        print(f\"🔄 Next step: Update 04_json_extraction.ipynb to use enhanced blocks\")\n",
    "        print(f\"   Change input directory to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of results\n",
    "if output_dir.exists():\n",
    "    enhanced_files = list(output_dir.glob(\"*.chunkr.json\"))\n",
    "    \n",
    "    if enhanced_files:\n",
    "        print(f\"📈 Analysis of {len(enhanced_files)} enhanced files:\\n\")\n",
    "        \n",
    "        total_original = 0\n",
    "        total_enhanced = 0\n",
    "        successful_chunkr = 0\n",
    "        \n",
    "        for file_path in enhanced_files[:5]:  # Analyze first 5 files\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                original_count = data.get('original_blocks_count', 0)\n",
    "                enhanced_count = data.get('enhanced_blocks_count', 0)\n",
    "                chunkr_success = data.get('chunkr_success', False)\n",
    "                \n",
    "                total_original += original_count\n",
    "                total_enhanced += enhanced_count\n",
    "                if chunkr_success:\n",
    "                    successful_chunkr += 1\n",
    "                \n",
    "                print(f\"📄 {file_path.name}:\")\n",
    "                print(f\"   Original blocks: {original_count}\")\n",
    "                print(f\"   Enhanced blocks: {enhanced_count}\")\n",
    "                print(f\"   Chunkr success: {'✅' if chunkr_success else '❌'}\")\n",
    "                \n",
    "                # Show sample enhanced block\n",
    "                blocks = data.get('blocks', [])\n",
    "                if blocks:\n",
    "                    sample_block = blocks[0]\n",
    "                    text = sample_block.get('text', '')[:60]\n",
    "                    semantic_type = sample_block.get('semantic_type', 'text')\n",
    "                    print(f\"   Sample: [{semantic_type}] {text}...\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error analyzing {file_path.name}: {e}\")\n",
    "        \n",
    "        print(f\"📊 Summary (first 5 files):\")\n",
    "        print(f\"   Total original blocks: {total_original}\")\n",
    "        print(f\"   Total enhanced blocks: {total_enhanced}\")\n",
    "        print(f\"   Successful Chunkr processing: {successful_chunkr}/5\")\n",
    "        \n",
    "        if total_original > 0:\n",
    "            ratio = total_enhanced / total_original\n",
    "            print(f\"   Enhancement ratio: {ratio:.2f}x\")\n",
    "    else:\n",
    "        print(\"📁 No enhanced files found in output directory\")\n",
    "else:\n",
    "    print(\"📁 Output directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b74843",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Update 04_json_extraction.ipynb**: Change the input directory to use enhanced blocks:\n",
    "   ```python\n",
    "   blocks_dir = \"outputs/run_001/03b_chunkr_enhanced\"\n",
    "   ```\n",
    "\n",
    "2. **Update file patterns**: Look for `.chunkr.json` files:\n",
    "   ```python\n",
    "   patterns = [\"page_*_blocks.chunkr.json\", \"page_*_blocks.llmcleaned.json\", ...]\n",
    "   ```\n",
    "\n",
    "3. **Leverage semantic metadata**: Use the enhanced semantic types and chunk IDs in your JSON extraction\n",
    "\n",
    "4. **Monitor performance**: Compare extraction quality with and without Chunkr enhancement\n",
    "\n",
    "## Benefits of Chunkr Enhancement\n",
    "\n",
    "- **Better chunking**: Content is semantically segmented rather than arbitrarily split\n",
    "- **Structure awareness**: Identifies tables, headings, paragraphs automatically\n",
    "- **Cross-reference resolution**: Maintains document flow and relationships\n",
    "- **Metadata enrichment**: Additional semantic information for downstream processing\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Service not available**: Run `./setup_local_chunkr.sh`\n",
    "- **Processing timeouts**: Increase `timeout_seconds` or reduce `chunk_target_length`\n",
    "- **Memory issues**: Process files in smaller batches\n",
    "- **Quality issues**: Adjust Chunkr configuration parameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
