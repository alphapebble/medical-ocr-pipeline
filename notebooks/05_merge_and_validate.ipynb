{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb34caad",
   "metadata": {},
   "source": [
    "# 05 — Merge & Validate\n",
    "\n",
    "Merge validated chunk JSONs into a single report; add lineage and metrics. Outputs final JSON + lineage + metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f572693",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "chunks_dir = \"outputs/run_001/04_jsonextracted\"\n",
    "output_dir = \"outputs/run_001/05_merged_validated\"\n",
    "reference_schema = \"config/schema_prescription.json\"\n",
    "dedupe_keys = [\"drug\", \"strength\", \"frequency\", \"timing\"]\n",
    "schema_key = \"prescription\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4826a7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No final_valid_* pieces found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m pieces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(chunks_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_valid_*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pieces:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo final_valid_* pieces found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge_dict\u001b[39m(a,b):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m b\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No final_valid_* pieces found"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "\n",
    "chunks_dir = Path(chunks_dir)\n",
    "out_dir = Path(output_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pieces = sorted(chunks_dir.glob(\"final_valid_*.json\"))\n",
    "\n",
    "if not pieces:\n",
    "    raise RuntimeError(\"No final_valid_* pieces found\")\n",
    "\n",
    "def merge_dict(a,b):\n",
    "    for k,v in b.items():\n",
    "        if k==\"metadata\": continue\n",
    "        if k not in a or a[k] in (\"\",None,[],{}): a[k]=v; continue\n",
    "        if isinstance(a[k],dict) and isinstance(v,dict): a[k]=merge_dict(a[k],v); continue\n",
    "        if isinstance(a[k],list) and isinstance(v,list):\n",
    "            seen=set(); out=[]\n",
    "            for it in (a[k]+v):\n",
    "                key = json.dumps(it, sort_keys=True) if isinstance(it,dict) else str(it)\n",
    "                if key in seen: continue\n",
    "                seen.add(key); out.append(it)\n",
    "            a[k]=out; continue\n",
    "    return a\n",
    "\n",
    "merged = {}\n",
    "for p in pieces:\n",
    "    try:\n",
    "        d = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        merged = merge_dict(merged, d)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Failed to merge piece:\", p, e)\n",
    "\n",
    "metrics = {\"pieces\": [p.name for p in pieces], \"total_pieces\": len(pieces), \"timestamp\": int(time.time())}\n",
    "lineage = {\"sources\": [str(p) for p in pieces]}\n",
    "\n",
    "final_path = out_dir / f\"final_{schema_key}.json\"\n",
    "lineage_path = out_dir / f\"final_{schema_key}_lineage.json\"\n",
    "metrics_path = out_dir / f\"final_{schema_key}_metrics.json\"\n",
    "\n",
    "final_path.write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "lineage_path.write_text(json.dumps(lineage, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "metrics_path.write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", final_path)\n",
    "print(\"Saved:\", lineage_path)\n",
    "print(\"Saved:\", metrics_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcd24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "/Users/balijepalli/miniconda3/envs/rxetl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved structured JSON → combined_rizwana_begum.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import easyocr\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# OCR + text extraction\n",
    "# ---------------------------\n",
    "def extract_pdf_text(pdf_path: str, use_easyocr=True):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    reader = easyocr.Reader([\"en\"], gpu=False) if use_easyocr else None\n",
    "\n",
    "    pages = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text_blocks = page.get_text(\"blocks\")\n",
    "        page_text = \" \".join(b[4] for b in text_blocks if len(b) > 4 and b[4].strip())\n",
    "\n",
    "        # OCR fallback for embedded images\n",
    "        ocr_texts = []\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            img_bytes = base_image[\"image\"]\n",
    "            if reader:\n",
    "                import numpy as np\n",
    "                import cv2\n",
    "                import io\n",
    "                from PIL import Image\n",
    "                pil_img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "                results = reader.readtext(np.array(pil_img))\n",
    "                ocr_texts.extend([res[1] for res in results])\n",
    "\n",
    "        full_text = page_text + \"\\n\" + \"\\n\".join(ocr_texts)\n",
    "        pages.append(full_text)\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "# ---------------------------\n",
    "# Simple rule-based structuring\n",
    "# ---------------------------\n",
    "def parse_reports(raw_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Very naive keyword-based splitter for demonstration.\n",
    "    In practice, you'd want regex / LLM cleanup.\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        \"histopathology\": \"\",\n",
    "        \"immunohistochemistry\": \"\",\n",
    "        \"radiology\": \"\"\n",
    "    }\n",
    "    current = None\n",
    "    for line in raw_text.splitlines():\n",
    "        low = line.lower()\n",
    "        if \"histopathology\" in low:\n",
    "            current = \"histopathology\"\n",
    "        elif \"immunohistochemistry\" in low or \"ihc\" in low:\n",
    "            current = \"immunohistochemistry\"\n",
    "        elif \"usg\" in low or \"ultrasonography\" in low or \"radiology\" in low:\n",
    "            current = \"radiology\"\n",
    "\n",
    "        if current:\n",
    "            sections[current] += line + \"\\n\"\n",
    "\n",
    "    combined = {\n",
    "        \"patient\": {},   # you can add regexes to capture name/age/sex\n",
    "        \"reports\": []\n",
    "    }\n",
    "    for key, text in sections.items():\n",
    "        if text.strip():\n",
    "            combined[\"reports\"].append({\n",
    "                \"report_type\": key,\n",
    "                \"raw_text\": text.strip()\n",
    "            })\n",
    "    return combined\n",
    "\n",
    "# ---------------------------\n",
    "# Main driver\n",
    "# ---------------------------\n",
    "def process_pdf_to_json(pdf_path: str, out_path: str):\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    structured = parse_reports(raw_text)\n",
    "    Path(out_path).write_text(json.dumps(structured, indent=2, ensure_ascii=False))\n",
    "    print(f\"Saved structured JSON → {out_path}\")\n",
    "\n",
    "# Example run\n",
    "if __name__ == \"__main__\":\n",
    "    process_pdf_to_json(\n",
    "        pdf_path = \"input_pdfs/ET1-Adobe Scan 10 Sept 2025.pdf\",\n",
    "        out_path=\"combined_rizwana_begum.json\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rxetl)",
   "language": "python",
   "name": "rxetl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
