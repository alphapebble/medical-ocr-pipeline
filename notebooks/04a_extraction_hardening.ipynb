{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e053bf0",
   "metadata": {},
   "source": [
    "# 04a — JSON Extraction Hardening\n",
    "Harden the final JSON extraction step:\n",
    "- Coerce any model output into a list of dicts\n",
    "- Save raw model replies per chunk\n",
    "- Page-by-page chunking + fallback model\n",
    "- Quick QA of produced valid chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77b2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_root = \"outputs/run_001\"\n",
    "schema_json = \"config/schema_prescription.json\"\n",
    "primary_model = \"alibayram/medgemma:latest\"\n",
    "fallback_model = \"mistral:7b\"\n",
    "chunk_size = 40\n",
    "max_retries = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87efa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using input folder → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/03_llmcleaned\n",
      "[INFO] Output folder → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/04_jsonextracted\n",
      "[INFO] Schema → schema_prescription.json\n"
     ]
    }
   ],
   "source": [
    "import re, json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "except Exception:\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "run_root = Path(run_root).expanduser().resolve()\n",
    "schema_path = Path(schema_json).expanduser().resolve()\n",
    "if not schema_path.exists():\n",
    "    raise FileNotFoundError(schema_path)\n",
    "\n",
    "out_dir = run_root / '04_jsonextracted'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "search_order = ['03_llmcleaned', '02_cleaned', '01_blocks']\n",
    "blocks_dir: Optional[Path] = None\n",
    "for folder in search_order:\n",
    "    candidate = run_root / folder\n",
    "    if any(candidate.glob('page_*_blocks*.json')):\n",
    "        blocks_dir = candidate\n",
    "        break\n",
    "if not blocks_dir:\n",
    "    raise FileNotFoundError(f'No page_* block files found under {run_root}')\n",
    "\n",
    "print('[INFO] Using input folder →', blocks_dir)\n",
    "print('[INFO] Output folder →', out_dir)\n",
    "print('[INFO] Schema →', schema_path.name)\n",
    "\n",
    "schema_str = schema_path.read_text(encoding='utf-8')\n",
    "schema = json.loads(schema_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c136e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def coerce_json_str(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```(?:json)?\\s*\", \"\", s)\n",
    "        s = re.sub(r\"\\s*```$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def coerce_to_list_of_dicts(obj):\n",
    "    if obj is None:\n",
    "        return []\n",
    "    if isinstance(obj, str):\n",
    "        s = obj.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "        except Exception:\n",
    "            return [{'_raw': s}]\n",
    "    if isinstance(obj, dict):\n",
    "        return [obj]\n",
    "    if isinstance(obj, list):\n",
    "        out = []\n",
    "        for x in obj:\n",
    "            out.append(x if isinstance(x, dict) else {'_value': x})\n",
    "        return out\n",
    "    return [{'_value': obj}]\n",
    "\n",
    "def is_effectively_empty(obj: dict) -> bool:\n",
    "    def empty(x):\n",
    "        return (x is None) or (isinstance(x, str) and x.strip()==\"\") or (isinstance(x, list) and len(x)==0)\n",
    "    keys_scalar = [\"diagnosis\",\"complaints\",\"advice\",\"follow_up\"]\n",
    "    top = [\"patient\",\"doctor\"]\n",
    "    if all(empty(obj.get(k)) for k in keys_scalar) \\\n",
    "       and all(empty(obj.get(k,{}).get(\"name\",\"\")) for k in top) \\\n",
    "       and empty(obj.get(\"medications\",[])) and empty(obj.get(\"tests\",[])) and empty(obj.get(\"investigations\",[])):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    'Return **only** a JSON **array** of objects that conforms to this schema.\\n'\n",
    "    'Do not include prose or code fences.\\n\\n'\n",
    "    'Schema:\\n{schema}\\n\\nBlocks (list of objects with bbox & text):\\n{blocks}\\n'\n",
    ")\n",
    "fmt = StrOutputParser()\n",
    "\n",
    "def make_llm(name: str):\n",
    "    if not name:\n",
    "        return None\n",
    "    try:\n",
    "        return ChatOllama(model=name, temperature=0, model_kwargs={'keep_alive': 0, 'format': 'json'})\n",
    "    except Exception as e:\n",
    "        print('[WARN] Could not init LLM', name, e)\n",
    "        return None\n",
    "\n",
    "def try_model(name: str, payload: str, max_retries: int, raw_outfile: Path):\n",
    "    llm = make_llm(name)\n",
    "    if not llm:\n",
    "        return None\n",
    "    chain = prompt | llm | fmt\n",
    "    cur_payload = payload\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            out = chain.invoke({'schema': schema_str, 'blocks': cur_payload})\n",
    "            raw = (out or '').strip()\n",
    "            raw_outfile.write_text(raw, encoding='utf-8')\n",
    "            if raw.startswith('```'):\n",
    "                import re\n",
    "                raw = re.sub(r'^```(?:json)?\\\\s*', '', raw)\n",
    "                raw = re.sub(r'\\\\s*```$', '', raw)\n",
    "            try:\n",
    "                parsed = json.loads(raw)\n",
    "            except Exception:\n",
    "                parsed = raw\n",
    "            as_list = coerce_to_list_of_dicts(parsed)\n",
    "            if not as_list or not any(isinstance(x, dict) and x for x in as_list):\n",
    "                raise ValueError('Parsed to empty or non-dict content')\n",
    "            return as_list\n",
    "        except Exception as e:\n",
    "            print(f'[WARN] {name} attempt {attempt+1}/{max_retries} failed: {e}')\n",
    "            try:\n",
    "                obj = json.loads(cur_payload)\n",
    "                shrunk = obj[:max(1, int(len(obj) * 0.75))]\n",
    "                cur_payload = json.dumps(shrunk, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def verify_valid_jsons(valid_dir: Path):\n",
    "    print(f\"\\n[QA] Verifying chunked valid JSONs in {valid_dir}...\")\n",
    "    total_records = 0\n",
    "    files = sorted(valid_dir.glob('valid_chunk_*.json')) + sorted(valid_dir.glob('*_valid_*.json'))\n",
    "    for f in files:\n",
    "        try:\n",
    "            data = json.loads(f.read_text(encoding='utf-8'))\n",
    "            if isinstance(data, dict):\n",
    "                sample_keys = list(data.keys())[:6]\n",
    "                print(f'  ✓ {f.name}: dict (1 record) keys={sample_keys}')\n",
    "                total_records += 1\n",
    "            elif isinstance(data, list):\n",
    "                recs = len(data)\n",
    "                sample = data[0] if recs else {}\n",
    "                sample_keys = list(sample.keys())[:6] if isinstance(sample, dict) else [type(sample).__name__]\n",
    "                print(f'  ✓ {f.name}: list ({recs} records) sample_keys={sample_keys}')\n",
    "                total_records += recs\n",
    "            else:\n",
    "                print(f'[WARN] {f.name}: unexpected type {type(data).__name__}')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] {f.name}: {e}')\n",
    "    print(f'[SUMMARY] Total merged records (pre-merge): {total_records}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6eecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 8 page files.\n",
      "\n",
      "[INFO] Page page_001_blocks.domain.llm: 8 blocks\n",
      "  ✓ chunk 1 ok → page_001_blocks.domain.llm_valid_1.json (items=1)\n",
      "\n",
      "[INFO] Page page_002_blocks.domain.llm: 12 blocks\n",
      "  ✓ chunk 1 ok → page_002_blocks.domain.llm_valid_1.json (items=1)\n",
      "\n",
      "[INFO] Page page_003_blocks.domain.llm: 8 blocks\n",
      "  ✓ chunk 1 ok → page_003_blocks.domain.llm_valid_1.json (items=1)\n",
      "\n",
      "[INFO] Page page_004_blocks.domain.llm: 9 blocks\n",
      "  ✓ chunk 1 ok → page_004_blocks.domain.llm_valid_1.json (items=1)\n",
      "\n",
      "[INFO] Page page_1_blocks.domain.llm: 56 blocks\n",
      "  ✓ chunk 1 ok → page_1_blocks.domain.llm_valid_1.json (items=1)\n",
      "  ✓ chunk 2 ok → page_1_blocks.domain.llm_valid_2.json (items=1)\n",
      "\n",
      "[INFO] Page page_2_blocks.domain.llm: 74 blocks\n",
      "  ✓ chunk 1 ok → page_2_blocks.domain.llm_valid_1.json (items=1)\n",
      "  ✓ chunk 2 ok → page_2_blocks.domain.llm_valid_2.json (items=1)\n",
      "\n",
      "[INFO] Page page_3_blocks.domain.llm: 17 blocks\n",
      "  ✓ chunk 1 ok → page_3_blocks.domain.llm_valid_1.json (items=1)\n",
      "\n",
      "[INFO] Page page_4_blocks.domain.llm: 30 blocks\n",
      "  ✓ chunk 1 ok → page_4_blocks.domain.llm_valid_1.json (items=1)\n"
     ]
    }
   ],
   "source": [
    "pages = sorted((run_root / '03_llmcleaned').glob('page_*_blocks*.json'))\n",
    "if not pages:\n",
    "    pages = sorted((run_root / '02_cleaned').glob('page_*_blocks*.json'))\n",
    "if not pages:\n",
    "    pages = sorted((run_root / '01_blocks').glob('page_*_blocks*.json'))\n",
    "if not pages:\n",
    "    raise FileNotFoundError('No page_* JSON files found')\n",
    "print(f'[INFO] Found {len(pages)} page files.')\n",
    "\n",
    "piece_paths = []\n",
    "for page_path in pages:\n",
    "    name = page_path.stem\n",
    "    blocks = json.loads(page_path.read_text(encoding='utf-8'))\n",
    "    print(f\"\\n[INFO] Page {name}: {len(blocks)} blocks\")\n",
    "    for idx, chunk in enumerate(chunks(blocks, int(chunk_size)), start=1):\n",
    "        payload = json.dumps([\n",
    "            {'bbox': b.get('bbox', [0,0,1,1]), 'text': b.get('text', ''), 'source': b.get('source', '')}\n",
    "            for b in chunk\n",
    "        ], ensure_ascii=False)\n",
    "        (out_dir / f'{name}_input_{idx}.json').write_text(payload, encoding='utf-8')\n",
    "        raw_path = out_dir / f'{name}_raw_{idx}.txt'\n",
    "        parsed = try_model(primary_model, payload, int(max_retries), raw_outfile=raw_path)\n",
    "        if parsed is None and fallback_model:\n",
    "            print(f'  [INFO] Retrying chunk {idx} with fallback {fallback_model}')\n",
    "            parsed = try_model(fallback_model, payload, int(max_retries), raw_outfile=raw_path)\n",
    "        if parsed is not None:\n",
    "            vp = out_dir / f'{name}_valid_{idx}.json'\n",
    "            vp.write_text(json.dumps(parsed, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "            piece_paths.append(vp)\n",
    "            print(f'  ✓ chunk {idx} ok → {vp.name} (items={len(parsed)})')\n",
    "        else:\n",
    "            print(f'  ✗ chunk {idx} failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a83131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🩺 merged → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/04_jsonextracted/final_prescription.json (records=10)\n",
      "\n",
      "[QA] Verifying chunked valid JSONs in /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/04_jsonextracted...\n",
      "  ✓ page_001_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_002_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_003_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_004_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_1_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_1_blocks.domain.llm_valid_2.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_2_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_2_blocks.domain.llm_valid_2.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_3_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "  ✓ page_4_blocks.domain.llm_valid_1.json: list (1 records) sample_keys=['_raw']\n",
      "[SUMMARY] Total merged records (pre-merge): 10\n",
      "\n",
      "✅ Extraction hardening complete.\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for p in piece_paths:\n",
    "    try:\n",
    "        obj = json.loads(p.read_text(encoding='utf-8'))\n",
    "        final.extend(coerce_to_list_of_dicts(obj))\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Skipping {p.name}: {e}')\n",
    "\n",
    "final_path = out_dir / 'final_prescription.json'\n",
    "final_path.write_text(json.dumps(final, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f\"\\n🩺 merged → {final_path} (records={len(final)})\")\n",
    "verify_valid_jsons(out_dir)\n",
    "print('\\n✅ Extraction hardening complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rxetl)",
   "language": "python",
   "name": "rxetl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
