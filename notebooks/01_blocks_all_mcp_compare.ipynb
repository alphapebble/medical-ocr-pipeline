{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348103eb",
   "metadata": {},
   "source": [
    "\n",
    "# 01 — PDF Blocks with MCP OCR (Tesseract, EasyOCR, Paddle, Surya)\n",
    "\n",
    "Compare multiple OCR engines **via MCP-friendly REST** (POST `/ocr`) on each PDF page.\n",
    "This notebook:\n",
    "- Extracts **native** text (PyMuPDF) for vector PDFs (fallback: pdf2image raster only)\n",
    "- Calls any configured **MCP OCR servers**:\n",
    "  - **Tesseract MCP** (`TESS_MCP_URL`)\n",
    "  - **EasyOCR MCP** (`EASYOCR_MCP_URL`)\n",
    "  - **PaddleOCR MCP** (`PADDLE_MCP_URL`)\n",
    "  - **Surya OCR MCP** (`SURYA_MCP_URL`)\n",
    "- Normalizes blocks, **merges & deduplicates**, saves per-page JSON and overlay PNGs\n",
    "- Writes a **comparison CSV**: per-engine page stats (blocks, chars, mean conf)\n",
    "\n",
    "> **Server contract**: Endpoint must accept `POST /ocr` with form `image=<file>, lang=<code>` and return JSON with either:\n",
    "> - `{\"text\": \"...\", \"avg_confidence\": 0.0}` (text-only), **or**\n",
    "> - `{\"blocks\":[{\"text\": \"...\", \"confidence\": 0.95, \"bbox\":[x0,y0,x1,y1]}, ...]}` (or `lines`/`results` with poly boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e54466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PDF: /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/input_pdfs/ET1-Adobe Scan 10 Sept 2025.pdf\n",
      "[INFO] Out: /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_mcp/01_blocks\n",
      "[INFO] Endpoints: {\n",
      "  \"tesseract\": \"http://127.0.0.1:8089/ocr\",\n",
      "  \"easyocr\": \"http://127.0.0.1:8092/ocr\",\n",
      "  \"paddle\": \"http://127.0.0.1:8090/ocr\",\n",
      "  \"surya\": \"http://127.0.0.1:8091/ocr\",\n",
      "  \"docling\": \"http://127.0.0.1:8093/ocr\"\n",
      "}\n",
      "[INFO] Warming up Docling...\n",
      "[WARN] Docling warmup failed: 500 Server Error: Internal Server Error for url: http://127.0.0.1:8093/warmup\n",
      "Health: {'tesseract': (False, 'disabled'), 'easyocr': (False, 'disabled'), 'paddle': (False, \"HTTPConnectionPool(host='127.0.0.1', port=8090): Max retries exceeded with url: /ocr (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1054b4c10>: Failed to establish a new connection: [Errno 61] Connection refused'))\"), 'surya': (False, 'disabled'), 'docling': (False, 'disabled')}\n",
      "\n",
      "[Page 1/4]\n",
      "  Summary: native=0 tess=0 easy=0 paddle=0 surya=0 docling=0 → merged=0\n",
      "\n",
      "[Page 2/4]\n",
      "  Summary: native=0 tess=0 easy=0 paddle=0 surya=0 docling=0 → merged=0\n",
      "\n",
      "[Page 3/4]\n",
      "  Summary: native=0 tess=0 easy=0 paddle=0 surya=0 docling=0 → merged=0\n",
      "\n",
      "[Page 4/4]\n",
      "  Summary: native=0 tess=0 easy=0 paddle=0 surya=0 docling=0 → merged=0\n",
      "\n",
      "✅ Done → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_mcp/01_blocks\n",
      "📄 CSV → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_mcp/01_blocks/engine_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 01 — PDF Blocks with MCP OCR (Tesseract, EasyOCR, Paddle, Surya)\n",
    "# \n",
    "# Compare multiple OCR engines via MCP-friendly REST (POST `/ocr`) on each PDF page.\n",
    "\n",
    "# %%\n",
    "# --------------------------- CONFIG ----------------------------------------\n",
    "import os, sys, json, time, base64, math, traceback\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import requests\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# PDF input & output\n",
    "pdf_path       = \"input_pdfs/ET1-Adobe Scan 10 Sept 2025.pdf\"\n",
    "output_dir     = \"outputs/run_mcp/01_blocks\"\n",
    "dpi            = 300\n",
    "ocr_lang       = \"en\"   # 'en','hi','te','mr','ta'\n",
    "\n",
    "# Which engines to try\n",
    "USE_NATIVE      = False\n",
    "USE_TESS_MCP    = False\n",
    "USE_EASYOCR_MCP = False\n",
    "USE_PADDLE_MCP  = True  \n",
    "USE_SURYA_MCP   = False\n",
    "USE_DOCLING_MCP = False\n",
    "\n",
    "# MCP endpoint URLs\n",
    "MCP_ENDPOINTS: Dict[str, Optional[str]] = {\n",
    "    \"tesseract\": os.getenv(\"TESS_MCP_URL\",    \"http://127.0.0.1:8089/ocr\").strip() or None,\n",
    "    \"easyocr\":   os.getenv(\"EASYOCR_MCP_URL\", \"http://127.0.0.1:8092/ocr\").strip() or None,\n",
    "    \"paddle\":    os.getenv(\"PADDLE_MCP_URL\",  \"http://127.0.0.1:8090/ocr\").strip() or None,\n",
    "    \"surya\":     os.getenv(\"SURYA_MCP_URL\",   \"http://127.0.0.1:8091/ocr\").strip() or None,\n",
    "    \"docling\":   os.getenv(\"DOCLING_MCP_URL\", \"http://127.0.0.1:8093/ocr\").strip() or None,\n",
    "}\n",
    "\n",
    "# Preprocessing\n",
    "mask_banners    = True\n",
    "banner_top_pct  = 0.18\n",
    "banner_bot_pct  = 0.20\n",
    "\n",
    "# Merge / Filtering\n",
    "min_conf        = 0.50\n",
    "line_join_px    = 14\n",
    "para_join_px    = 26\n",
    "dedup_iou_thr   = 0.50\n",
    "dedup_sim_thr   = 0.92\n",
    "native_len_gate = 100\n",
    "\n",
    "# Visualization\n",
    "make_viz_png    = True\n",
    "\n",
    "# I/O\n",
    "out_dir = Path(output_dir).expanduser().resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[INFO] PDF:\", Path(pdf_path).expanduser().resolve())\n",
    "print(\"[INFO] Out:\", out_dir)\n",
    "print(\"[INFO] Endpoints:\", json.dumps(MCP_ENDPOINTS, indent=2))\n",
    "\n",
    "# %%\n",
    "# --------------------------- IMPORTS & SETUP -------------------------------\n",
    "import io, json, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "try:\n",
    "    import fitz as _fitz\n",
    "    HAVE_FITZ = True\n",
    "except:\n",
    "    HAVE_FITZ = False\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "except:\n",
    "    from difflib import SequenceMatcher\n",
    "    def fuzz_ratio(a,b): return int(100*SequenceMatcher(None, a, b).ratio())\n",
    "\n",
    "LANG_MAP = {\n",
    "    \"en\":[\"en\"], \"hi\":[\"hi\",\"en\"], \"te\":[\"te\",\"en\"], \"mr\":[\"mr\",\"en\"], \"ta\":[\"ta\",\"en\"]\n",
    "}\n",
    "langs = LANG_MAP.get(ocr_lang.lower(), [\"en\"])\n",
    "\n",
    "# %%\n",
    "# ------------------------------ UTILITIES ----------------------------------\n",
    "def page_to_image(doc, page_index: int, dpi: int=300) -> Image.Image:\n",
    "    page = doc[page_index]\n",
    "    zoom = dpi / 72\n",
    "    mat  = _fitz.Matrix(zoom, zoom)\n",
    "    pix  = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    return Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "def mask_bands(pil: Image.Image, top_pct: float, bot_pct: float) -> Image.Image:\n",
    "    if not mask_banners:\n",
    "        return pil\n",
    "    w,h   = pil.size\n",
    "    top_h = int(h * max(0, min(0.45, top_pct)))\n",
    "    bot_h = int(h * max(0, min(0.45, bot_pct)))\n",
    "    out   = pil.copy()\n",
    "    draw  = ImageDraw.Draw(out)\n",
    "    bg = (240,240,240)\n",
    "    if top_h>0: draw.rectangle([0,0,w,top_h], fill=bg)\n",
    "    if bot_h>0: draw.rectangle([0,h-bot_h,w,h], fill=bg)\n",
    "    return out\n",
    "\n",
    "def blocks_sort_key(b):\n",
    "    y0 = round(b[\"bbox\"][1],1); x0 = round(b[\"bbox\"][0],1)\n",
    "    return (y0, x0)\n",
    "\n",
    "def iou(a, b) -> float:\n",
    "    ax0, ay0, ax1, ay1 = a; bx0, by0, bx1, by1 = b\n",
    "    inter_x0 = max(ax0, bx0); inter_y0 = max(ay0, by0)\n",
    "    inter_x1 = min(ax1, bx1); inter_y1 = min(ay1, by1)\n",
    "    iw = max(0.0, inter_x1 - inter_x0); ih = max(0.0, inter_y1 - inter_y0)\n",
    "    inter = iw * ih\n",
    "    if inter <= 0: return 0.0\n",
    "    area_a = (ax1-ax0)*(ay1-ay0); area_b = (bx1-bx0)*(by1-by0)\n",
    "    return inter / max(1e-6, area_a + area_b - inter)\n",
    "\n",
    "def section_of(bbox, page_h):\n",
    "    cy = 0.5 * (bbox[1] + bbox[3])\n",
    "    if cy < 0.25*page_h: return \"header\"\n",
    "    if cy > 0.85*page_h: return \"footer\"\n",
    "    return \"body\"\n",
    "\n",
    "# %%\n",
    "# ------------------------------ NATIVE -------------------------------------\n",
    "def extract_native(page) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    for b in page.get_text(\"blocks\"):\n",
    "        if len(b) >= 5:\n",
    "            x0,y0,x1,y1,txt = b[:5]\n",
    "            if txt and str(txt).strip():\n",
    "                out.append({\n",
    "                    \"bbox\":[float(x0),float(y0),float(x1),float(y1)],\n",
    "                    \"text\":str(txt).strip(),\n",
    "                    \"source\":\"native\",\n",
    "                    \"confidence\":1.0\n",
    "                })\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "# %%\n",
    "# ------------------------------ MCP CLIENTS --------------------------------\n",
    "def pick_lang(langs):\n",
    "    supported = {\"en\",\"hi\",\"te\",\"mr\",\"ta\"}\n",
    "    return next((l for l in langs if l in supported), \"en\")\n",
    "\n",
    "def parse_mcp_json(js, w, h, source_tag=\"mcp\"):\n",
    "    \"\"\"Parse MCP OCR response into normalized blocks.\"\"\"\n",
    "    out = []\n",
    "\n",
    "    def to_bbox(bb):\n",
    "        if not bb:\n",
    "            return (0,0,w,h)\n",
    "        # [x0,y0,x1,y1]\n",
    "        if isinstance(bb, (list, tuple)) and len(bb) == 4 and all(isinstance(v,(int,float)) for v in bb):\n",
    "            return tuple(float(x) for x in bb)\n",
    "        # flat polygon [x0,y0,x1,y1,x2,y2,x3,y3]\n",
    "        if isinstance(bb, (list, tuple)) and len(bb) == 8 and all(isinstance(v,(int,float)) for v in bb):\n",
    "            xs, ys = bb[0::2], bb[1::2]\n",
    "            return (float(min(xs)), float(min(ys)), float(max(xs)), float(max(ys)))\n",
    "        # list of [x,y] points\n",
    "        if isinstance(bb, (list, tuple)) and bb and isinstance(bb[0], (list, tuple)) and len(bb[0]) == 2:\n",
    "            xs = [p[0] for p in bb]; ys = [p[1] for p in bb]\n",
    "            return (float(min(xs)), float(min(ys)), float(max(xs)), float(max(ys)))\n",
    "        return (0,0,w,h)\n",
    "\n",
    "    def norm_conf(c):\n",
    "        try:\n",
    "            c = float(c)\n",
    "        except:\n",
    "            c = 0.0\n",
    "        if c > 1.0: c = c / 100.0\n",
    "        return max(0.0, min(1.0, c))\n",
    "\n",
    "    # Check for blocks/lines/results containers\n",
    "    for key in (\"blocks\",\"lines\",\"results\",\"predictions\",\"preds\",\"data\"):\n",
    "        arr = js.get(key) if isinstance(js, dict) else None\n",
    "        if isinstance(arr, list) and arr:\n",
    "            for item in arr:\n",
    "                if isinstance(item, dict):\n",
    "                    txt  = item.get(\"text\", \"\").strip()\n",
    "                    # Remove HTML tags like <br>\n",
    "                    txt = txt.replace(\"<br>\", \" \").replace(\"<BR>\", \" \")\n",
    "                    conf = item.get(\"confidence\", item.get(\"score\", 1.0))\n",
    "                    box  = item.get(\"bbox\", item.get(\"box\", item.get(\"points\", item.get(\"polygon\"))))\n",
    "                    \n",
    "                    if txt and box is not None:\n",
    "                        x0,y0,x1,y1 = to_bbox(box)\n",
    "                        out.append({\n",
    "                            \"bbox\": [x0,y0,x1,y1],\n",
    "                            \"text\": txt,\n",
    "                            \"confidence\": norm_conf(conf),\n",
    "                            \"source\": source_tag\n",
    "                        })\n",
    "            return out\n",
    "\n",
    "    # Fallback: plain text\n",
    "    if isinstance(js, dict) and js.get(\"text\"):\n",
    "        txt = js.get(\"text\",\"\").strip()\n",
    "        if txt:\n",
    "            out.append({\n",
    "                \"bbox\": [0,0,w,h],\n",
    "                \"text\": txt,\n",
    "                \"confidence\": norm_conf(js.get(\"avg_confidence\", 0.0)),\n",
    "                \"source\": source_tag\n",
    "            })\n",
    "\n",
    "    return out\n",
    "\n",
    "def mcp_ocr(pil_img, url, langs, tag, engine_tag=None):\n",
    "    \"\"\"Call MCP OCR endpoint and return normalized blocks.\"\"\"\n",
    "    if not url:\n",
    "        return []\n",
    "    try:\n",
    "        # Per-engine timeouts: longer for heavy models like docling\n",
    "        timeout = 600 if engine_tag == \"docling\" else 120\n",
    "        \n",
    "        buf = io.BytesIO()\n",
    "        pil_img.save(buf, format=\"PNG\")\n",
    "        buf.seek(0)\n",
    "        payload = {\"lang\": pick_lang(langs)}\n",
    "        files = {\"image\": (\"page.png\", buf.getvalue(), \"image/png\")}\n",
    "        r = requests.post(url, data=payload, files=files, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        \n",
    "        # Debug logging\n",
    "        if isinstance(js, dict) and \"blocks\" in js:\n",
    "            print(f\"  [{tag}] Server returned {len(js['blocks'])} blocks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] {tag} request failed: {e}\")\n",
    "        return []\n",
    "    \n",
    "    w,h = pil_img.size\n",
    "    out = parse_mcp_json(js, w, h, source_tag=tag)\n",
    "    print(f\"  [{tag}] After parsing: {len(out)} blocks\")\n",
    "    \n",
    "    # Confidence filter\n",
    "    out = [b for b in out if b.get(\"text\") and float(b.get(\"confidence\",0.0)) >= min_conf]\n",
    "    print(f\"  [{tag}] After filtering (>={min_conf}): {len(out)} blocks\")\n",
    "    \n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "# %%\n",
    "# --------------------------- POST-PROCESSING --------------------------------\n",
    "def regroup_lines(blocks: List[Dict[str,Any]], line_gap:int=14, para_gap:int=26)->List[Dict[str,Any]]:\n",
    "    if not blocks: return []\n",
    "    bs = sorted(blocks, key=blocks_sort_key)\n",
    "    rows, cur = [], [bs[0]]\n",
    "    for b in bs[1:]:\n",
    "        prev = cur[-1]\n",
    "        if abs(b[\"bbox\"][1] - prev[\"bbox\"][1]) <= line_gap:\n",
    "            cur.append(b)\n",
    "        else:\n",
    "            rows.append(cur); cur=[b]\n",
    "    rows.append(cur)\n",
    "\n",
    "    lines=[]\n",
    "    for row in rows:\n",
    "        row = sorted(row, key=lambda x:x[\"bbox\"][0])\n",
    "        text = \" \".join(x[\"text\"] for x in row if x[\"text\"])\n",
    "        x0 = min(x[\"bbox\"][0] for x in row); y0=min(x[\"bbox\"][1] for x in row)\n",
    "        x1 = max(x[\"bbox\"][2] for x in row); y1=max(x[\"bbox\"][3] for x in row)\n",
    "        src= \"+\".join(sorted(set(x[\"source\"] for x in row)))\n",
    "        conf=sum(x.get(\"confidence\",1.0) for x in row)/len(row)\n",
    "        lines.append({\"bbox\":[x0,y0,x1,y1], \"text\":text.strip(), \"source\":src, \"confidence\":conf})\n",
    "\n",
    "    paras=[]\n",
    "    current=[lines[0]] if lines else []\n",
    "    for ln in lines[1:]:\n",
    "        prev=current[-1]\n",
    "        if abs(ln[\"bbox\"][1]-prev[\"bbox\"][3]) <= para_gap:\n",
    "            current.append(ln)\n",
    "        else:\n",
    "            txt=\" \".join(x[\"text\"] for x in current if x[\"text\"])\n",
    "            x0=min(x[\"bbox\"][0] for x in current); y0=min(x[\"bbox\"][1] for x in current)\n",
    "            x1=max(x[\"bbox\"][2] for x in current); y1=max(x[\"bbox\"][3] for x in current)\n",
    "            src=\"+\".join(sorted(set(\",\".join(x[\"source\"] for x in current).split(\"+\"))))\n",
    "            conf=sum(x.get(\"confidence\",1.0) for x in current)/len(current)\n",
    "            paras.append({\"bbox\":[x0,y0,x1,y1], \"text\":txt.strip(), \"source\":src, \"confidence\":conf})\n",
    "            current=[ln]\n",
    "    if current:\n",
    "        txt=\" \".join(x[\"text\"] for x in current)\n",
    "        x0=min(x[\"bbox\"][0] for x in current); y0=min(x[\"bbox\"][1] for x in current)\n",
    "        x1=max(x[\"bbox\"][2] for x in current); y1=max(x[\"bbox\"][3] for x in current)\n",
    "        src=\"+\".join(sorted(set(\",\".join(x[\"source\"] for x in current).split(\"+\"))))\n",
    "        conf=sum(x.get(\"confidence\",1.0) for x in current)/len(current)\n",
    "        paras.append({\"bbox\":[x0,y0,x1,y1], \"text\":txt.strip(), \"source\":src, \"confidence\":conf})\n",
    "    return paras\n",
    "\n",
    "def deduplicate(blocks: List[Dict[str,Any]], iou_thr:float=0.45, sim_thr:float=0.90)->List[Dict[str,Any]]:\n",
    "    out=[]\n",
    "    for b in sorted(blocks, key=lambda x: (-x.get(\"confidence\",1.0), len(x.get(\"text\",\"\")))):\n",
    "        t = (b.get(\"text\",\"\") or \"\").strip()\n",
    "        if not t: continue\n",
    "        keep=True\n",
    "        for a in out:\n",
    "            if iou(b[\"bbox\"], a[\"bbox\"]) >= iou_thr:\n",
    "                if fuzz_ratio(t.lower(), a[\"text\"].lower())/100.0 >= sim_thr:\n",
    "                    keep=False; break\n",
    "        if keep: out.append(b)\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "def merge_ensemble(native: List[Dict], ocrs: List[List[Dict]], page_h: int) -> List[Dict]:\n",
    "    all_blocks = []\n",
    "    all_blocks.extend(native)\n",
    "    for s in ocrs:\n",
    "        all_blocks.extend(s)\n",
    "    regrouped = regroup_lines(all_blocks, line_join_px, para_join_px)\n",
    "    deduped   = deduplicate(regrouped, dedup_iou_thr, dedup_sim_thr)\n",
    "    for b in deduped:\n",
    "        b[\"section\"] = section_of(b[\"bbox\"], page_h)\n",
    "    return deduped\n",
    "\n",
    "# %%\n",
    "# ------------------------------- HEALTH CHECK -------------------------------\n",
    "def ping_endpoint(url, langs=langs, engine_tag=None):\n",
    "    if not url: \n",
    "        return False, \"unset\"\n",
    "    try:\n",
    "        # Per-engine timeouts: longer for heavy models like docling\n",
    "        timeout = 300 if engine_tag == \"docling\" else 10\n",
    "        \n",
    "        img = Image.new(\"RGB\", (32, 32), (255,255,255))\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"PNG\")\n",
    "        buf.seek(0)\n",
    "        r = requests.post(url, data={\"lang\": pick_lang(langs)}, \n",
    "                         files={\"image\":(\"tiny.png\", buf.getvalue(), \"image/png\")}, \n",
    "                         timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        _ = r.json()\n",
    "        return True, \"ok\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Warmup Docling if endpoint available (pre-loads model)\n",
    "if MCP_ENDPOINTS.get(\"docling\"):\n",
    "    try:\n",
    "        print(\"[INFO] Warming up Docling...\")\n",
    "        warmup_url = MCP_ENDPOINTS[\"docling\"].rstrip(\"/ocr\") + \"/warmup\"\n",
    "        r = requests.get(warmup_url, timeout=600)\n",
    "        r.raise_for_status()\n",
    "        warmup_js = r.json()\n",
    "        if warmup_js.get(\"ok\"):\n",
    "            print(f\"[INFO] Docling warmup OK in {warmup_js.get('seconds', 'N/A')}s\")\n",
    "        else:\n",
    "            print(f\"[WARN] Docling warmup returned {warmup_js}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Docling warmup failed: {e}\")\n",
    "\n",
    "health = {\n",
    "    \"tesseract\": ping_endpoint(MCP_ENDPOINTS[\"tesseract\"], engine_tag=\"tesseract\") if USE_TESS_MCP and MCP_ENDPOINTS.get(\"tesseract\") else (False, \"disabled\"),\n",
    "    \"easyocr\":   ping_endpoint(MCP_ENDPOINTS[\"easyocr\"], engine_tag=\"easyocr\") if USE_EASYOCR_MCP and MCP_ENDPOINTS.get(\"easyocr\") else (False, \"disabled\"),\n",
    "    \"paddle\":    ping_endpoint(MCP_ENDPOINTS[\"paddle\"], engine_tag=\"paddle\") if USE_PADDLE_MCP and MCP_ENDPOINTS.get(\"paddle\") else (False, \"disabled\"),\n",
    "    \"surya\":     ping_endpoint(MCP_ENDPOINTS[\"surya\"], engine_tag=\"surya\") if USE_SURYA_MCP and MCP_ENDPOINTS.get(\"surya\") else (False, \"disabled\"),\n",
    "    \"docling\":   ping_endpoint(MCP_ENDPOINTS[\"docling\"], engine_tag=\"docling\") if USE_DOCLING_MCP and MCP_ENDPOINTS.get(\"docling\") else (False, \"disabled\"),\n",
    "}\n",
    "print(\"Health:\", health)\n",
    "\n",
    "# %%\n",
    "# --------------------------------- MAIN -------------------------------------\n",
    "pdf_path_p = Path(pdf_path).expanduser().resolve()\n",
    "\n",
    "if HAVE_FITZ:\n",
    "    doc = _fitz.open(pdf_path_p)\n",
    "    total_pages = len(doc)\n",
    "else:\n",
    "    raise RuntimeError(\"PyMuPDF required\")\n",
    "\n",
    "meta = {\n",
    "    \"pages\": total_pages,\n",
    "    \"dpi\": dpi,\n",
    "    \"langs\": langs,\n",
    "    \"mcp_endpoints\": MCP_ENDPOINTS\n",
    "}\n",
    "(out_dir/\"metadata.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def page_metrics(blocks):\n",
    "    if not blocks: return 0, 0.0, 0\n",
    "    chars = sum(len(b.get(\"text\",\"\")) for b in blocks)\n",
    "    confs = [float(b.get(\"confidence\",0.0)) for b in blocks if \"confidence\" in b]\n",
    "    mean_conf = (sum(confs)/len(confs)) if confs else 0.0\n",
    "    return chars, mean_conf, len(blocks)\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for i in range(total_pages):\n",
    "    print(f\"\\n[Page {i+1}/{total_pages}]\")\n",
    "    \n",
    "    # Native + raster\n",
    "    page = doc[i]\n",
    "    native = extract_native(page) if USE_NATIVE else []\n",
    "    (out_dir/f\"page_{i+1:03d}_native.json\").write_text(json.dumps(native,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    pil = page_to_image(doc, i, dpi=dpi)\n",
    "\n",
    "    # Preprocess\n",
    "    pil_base = mask_bands(pil, banner_top_pct, banner_bot_pct) if mask_banners else pil\n",
    "\n",
    "    # Call each MCP engine (force docling if endpoint exists, even if health failed)\n",
    "    o_tess = mcp_ocr(pil_base, MCP_ENDPOINTS[\"tesseract\"], langs, \"tesseract_mcp\", engine_tag=\"tesseract\") if (USE_TESS_MCP and health[\"tesseract\"][0]) else []\n",
    "    o_easy = mcp_ocr(pil_base, MCP_ENDPOINTS[\"easyocr\"],   langs, \"easyocr_mcp\", engine_tag=\"easyocr\") if (USE_EASYOCR_MCP and health[\"easyocr\"][0]) else []\n",
    "    o_padl = mcp_ocr(pil_base, MCP_ENDPOINTS[\"paddle\"],    langs, \"paddle_mcp\", engine_tag=\"paddle\") if (USE_PADDLE_MCP and health[\"paddle\"][0]) else []\n",
    "    o_sur  = mcp_ocr(pil_base, MCP_ENDPOINTS[\"surya\"],     langs, \"surya_mcp\", engine_tag=\"surya\") if (USE_SURYA_MCP and health[\"surya\"][0]) else []\n",
    "    o_doc  = mcp_ocr(pil_base, MCP_ENDPOINTS[\"docling\"],   langs, \"docling_mcp\", engine_tag=\"docling\") if USE_DOCLING_MCP and MCP_ENDPOINTS.get(\"docling\") else []\n",
    "\n",
    "    # Save raw per engine\n",
    "    if o_tess: (out_dir/f\"page_{i+1:03d}_ocr_tesseract_mcp.json\").write_text(json.dumps(o_tess,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if o_easy: (out_dir/f\"page_{i+1:03d}_ocr_easyocr_mcp.json\").write_text(json.dumps(o_easy,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if o_padl: (out_dir/f\"page_{i+1:03d}_ocr_paddle_mcp.json\").write_text(json.dumps(o_padl,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if o_sur:  (out_dir/f\"page_{i+1:03d}_ocr_surya_mcp.json\").write_text(json.dumps(o_sur,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if o_doc:  (out_dir/f\"page_{i+1:03d}_ocr_docling_mcp.json\").write_text(json.dumps(o_doc,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # Merge\n",
    "    ocr_heads = [o_tess, o_easy, o_padl, o_sur, o_doc]\n",
    "    native_chars = sum(len(b.get(\"text\",\"\")) for b in native)\n",
    "    merged = merge_ensemble(native if native_chars >= native_len_gate else [], ocr_heads, pil.height)\n",
    "\n",
    "    # Save merged\n",
    "    (out_dir / f\"page_{i+1:03d}_blocks.json\").write_text(json.dumps(merged, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # Visualization\n",
    "    if make_viz_png:\n",
    "        im = pil.copy()\n",
    "        dr = ImageDraw.Draw(im, \"RGBA\")\n",
    "        for b in merged:\n",
    "            x0,y0,x1,y1 = map(int, b[\"bbox\"])\n",
    "            src = (b.get(\"source\") or \"\").lower()\n",
    "            col = (122,199,136,80)\n",
    "            if \"native\"       in src: col=(147,112,219,90)   # purple\n",
    "            if \"tesseract_mcp\"in src: col=(70,130,180,90)    # steelblue\n",
    "            if \"easyocr_mcp\"  in src: col=(255,165,0,90)     # orange\n",
    "            if \"paddle_mcp\"   in src: col=(34,139,34,90)     # green\n",
    "            if \"surya_mcp\"    in src: col=(220,20,60,90)     # crimson\n",
    "            if \"docling_mcp\"  in src: col=(255,215,0,90)     # gold\n",
    "            dr.rectangle([x0,y0,x1,y1], outline=(0,0,0,180), width=2, fill=col)\n",
    "        im.save(out_dir/f\"page_{i+1:03d}_viz.png\")\n",
    "\n",
    "    # Metrics\n",
    "    for name, blocks in [\n",
    "        (\"native\", native),\n",
    "        (\"tesseract_mcp\", o_tess),\n",
    "        (\"easyocr_mcp\", o_easy),\n",
    "        (\"paddle_mcp\", o_padl),\n",
    "        (\"surya_mcp\", o_sur),\n",
    "        (\"docling_mcp\", o_doc),\n",
    "        (\"merged\", merged),\n",
    "    ]:\n",
    "        chars, mean_conf, n = page_metrics(blocks)\n",
    "        summary_rows.append({\n",
    "            \"page\": i+1, \"engine\": name, \"n_blocks\": n, \"chars\": chars, \"mean_conf\": round(mean_conf,4)\n",
    "        })\n",
    "\n",
    "    print(f\"  Summary: native={len(native)} tess={len(o_tess)} easy={len(o_easy)} \"\n",
    "          f\"paddle={len(o_padl)} surya={len(o_sur)} docling={len(o_doc)} → merged={len(merged)}\")\n",
    "\n",
    "# Write CSV\n",
    "import csv\n",
    "csv_path = out_dir / \"engine_comparison.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"page\",\"engine\",\"n_blocks\",\"chars\",\"mean_conf\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(summary_rows)\n",
    "\n",
    "print(f\"\\n✅ Done → {out_dir}\")\n",
    "print(f\"📄 CSV → {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9a813eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending image: (2200, 3300)\n",
      "Status: 200\n",
      "Response: {\n",
      "  \"engine\": \"surya\",\n",
      "  \"blocks\": [\n",
      "    {\n",
      "      \"text\": \"\\u0c05\\u0c16\\u0c3f\\u0c32 \\u0c2d\\u0c3e\\u0c30\\u0c24 \\u0c35\\u0c48\\u0c26\\u0c4d\\u0c2f \\u0c35\\u0c3f\\u0c1c\\u0c4d\\u0c1e\\u0c3e\\u0c28 \\u0c38\\u0c02\\u0c38\\u0c4d\\u0c25<br>\\u0905\\u0916\\u093f\\u0932 \\u092d\\u093e\\u0930\\u0924\\u0940\\u092f \\u0906\\u092f\\u0941\\u0930\\u094d\\u0935\\u093f\\u091c\\u094d\\u091e\\u093e\\u0928 \\u0938\\u0902\\u0938\\u094d\\u0925\\u093e\\u0928, \\u092c\\u0940\\u092c\\u0940\\u0928\\u0917\\u0930\",\n",
      "      \"confidence\": 0.9953843077023824,\n",
      "      \"bbox\": [\n",
      "        781.0,\n",
      "        193.0,\n",
      "        1538.0,\n",
      "        314.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"All India Institute of Medical Sciences, Bibinagar\",\n",
      "      \"confidence\": 0.998117997096135,\n",
      "      \"bbox\": [\n",
      "        665.0,\n",
      "        296.0,\n",
      "        1607.0,\n",
      "        362.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Hyderabad Metropolitan Region (HMR), Telangana-508126, India\",\n",
      "      \"confidence\": 0.9985745116587608,\n",
      "      \"bbox\": [\n",
      "        693.0,\n",
      "        376.0,\n",
      "        1607.0,\n",
      "        412.0\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "    \n",
      "Blocks found: 41\n",
      "First block: {'text': 'అఖిల భారత వైద్య విజ్ఞాన సంస్థ<br>अखिल भारतीय आयुर्विज्ञान संस्थान, बीबीनगर', 'confidence': 0.9953843077023824, 'bbox': [781.0, 193.0, 1538.0, 314.0]}\n"
     ]
    }
   ],
   "source": [
    "# Test with actual PDF page\n",
    "import io, requests, json\n",
    "from PIL import Image\n",
    "import fitz\n",
    "\n",
    "# Get first page as image\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc[0]\n",
    "zoom = dpi / 72\n",
    "mat = fitz.Matrix(zoom, zoom)\n",
    "pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "# Send to Surya\n",
    "buf = io.BytesIO()\n",
    "img.save(buf, format=\"PNG\")\n",
    "buf.seek(0)\n",
    "\n",
    "print(f\"Sending image: {img.size}\")\n",
    "r = requests.post(MCP_ENDPOINTS[\"surya\"],\n",
    "                  data={\"lang\":\"en\"},\n",
    "                  files={\"image\":(\"page.png\", buf.getvalue(), \"image/png\")},\n",
    "                  timeout=120)\n",
    "print(f\"Status: {r.status_code}\")\n",
    "result = r.json()\n",
    "print(f\"Response: {json.dumps(result, indent=2)[:1000]}\")\n",
    "print(f\"Blocks found: {len(result.get('blocks', []))}\")\n",
    "if result.get('blocks'):\n",
    "    print(f\"First block: {result['blocks'][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rxetl)",
   "language": "python",
   "name": "rxetl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
