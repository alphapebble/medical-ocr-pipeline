{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4374d4b8",
   "metadata": {},
   "source": [
    "# Notebook 2 — **Domain Cleanup + Medical Normalization + QA** (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== CONFIG (edit these) ====\n",
    "blocks_dir = \"outputs/run_001/01_blocks\"     # input folder from Notebook 1\n",
    "output_dir = \"outputs/run_001/02_cleaned\"    # output folder for this step\n",
    "dictionary_path = \"config/medical_terms.yml\" # optional YAML mapping\n",
    "fuzzy_cutoff = 0.86\n",
    "max_corrections = 200\n",
    "\n",
    "# Optional: QuickUMLS install path (folder with QuickUMLS data) — leave empty if not available\n",
    "quickumls_path = \"\"  # e.g., \"/data/QuickUMLS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a0cfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Could not load en_core_sci_lg: [E050] Can't find model 'en_core_sci_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "[WARN] Could not load en_ner_bc5cdr_md: [E050] Can't find model 'en_ner_bc5cdr_md'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "[WARN] Could not load en_ner_bionlp13cg_md: [E050] Can't find model 'en_ner_bionlp13cg_md'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "[INFO] QuickUMLS disabled (no path provided)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json, difflib, yaml, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    _HAS_SPACY = True\n",
    "except Exception:\n",
    "    _HAS_SPACY = False\n",
    "\n",
    "try:\n",
    "    import scispacy  # noqa: F401\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_NLP = None\n",
    "_LINKER = None\n",
    "if _HAS_SPACY:\n",
    "    for model in [\"en_core_sci_lg\", \"en_ner_bc5cdr_md\", \"en_ner_bionlp13cg_md\"]:\n",
    "        try:\n",
    "            _NLP = spacy.load(model)\n",
    "            print(f\"[INFO] Loaded spaCy model: {model}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not load {model}: {e}\")\n",
    "    if _NLP is not None:\n",
    "        try:\n",
    "            _NLP.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "            _LINKER = _NLP.get_pipe(\"scispacy_linker\")\n",
    "            print(\"[INFO] scispaCy UMLS linker enabled\")\n",
    "        except Exception as e:\n",
    "            _LINKER = None\n",
    "            print(\"[WARN] scispaCy linker not available:\", e)\n",
    "\n",
    "_MATCHER = None\n",
    "QUICKUMLS_PATH = os.environ.get(\"QUICKUMLS_PATH\", quickumls_path or \"\")\n",
    "if QUICKUMLS_PATH and Path(QUICKUMLS_PATH).exists():\n",
    "    try:\n",
    "        from quickumls import QuickUMLS\n",
    "        _MATCHER = QuickUMLS(QUICKUMLS_PATH)\n",
    "        print(f\"[INFO] QuickUMLS initialized from: {QUICKUMLS_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] QuickUMLS not usable:\", e)\n",
    "        _MATCHER = None\n",
    "else:\n",
    "    if QUICKUMLS_PATH:\n",
    "        print(f\"[WARN] QuickUMLS path not found: {QUICKUMLS_PATH}\")\n",
    "    else:\n",
    "        print(\"[INFO] QuickUMLS disabled (no path provided)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd877d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Input: /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/01_blocks\n",
      "[INFO] Output: /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/02_cleaned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blocks_dir = Path(blocks_dir).expanduser().resolve()\n",
    "if not blocks_dir.exists():\n",
    "    raise FileNotFoundError(f\"Input folder not found: {blocks_dir}\")\n",
    "out_root = Path(output_dir).expanduser().resolve()\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "qa_dir = out_root / \"_qa\"\n",
    "qa_dir.mkdir(exist_ok=True)\n",
    "print(\"[INFO] Input:\", blocks_dir)\n",
    "print(\"[INFO] Output:\", out_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d4dc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 8 extra dict terms from config/medical_terms.yml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BUILTIN_MAP = {\n",
    "    \"toabeculas\": \"trabeculae\",\"trabeculas\": \"trabeculae\",\"trabaculae\": \"trabeculae\",\n",
    "    \"fotymoyphoys\": \"polymorphous\",\"palato\": \"palate\",\"pala\": \"palate\",\"comozd\": \"composed\",\n",
    "    \"necrosls\": \"necrosis\",\"microscoplc\": \"microscopic\",\"cribriforme\": \"cribriform\",\n",
    "}\n",
    "try:\n",
    "    yml = Path(dictionary_path)\n",
    "    if yml.exists():\n",
    "        ext_map = yaml.safe_load(yml.read_text(encoding=\"utf-8\")) or {}\n",
    "        if isinstance(ext_map, dict):\n",
    "            BUILTIN_MAP.update(ext_map)\n",
    "            print(f\"[INFO] Loaded {len(ext_map)} extra dict terms from {yml}\")\n",
    "    else:\n",
    "        print(f\"[INFO] No external dictionary at {yml}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not load YAML dictionary:\", e)\n",
    "\n",
    "VOCAB = sorted(set(BUILTIN_MAP.values()).union({\n",
    "    \"histopathology\",\"specimen\",\"gross\",\"microscopic\",\"impression\",\"pleomorphic\",\n",
    "    \"trabeculae\",\"necrosis\",\"cribriform\",\"carcinoma\",\"fibrosis\",\"lymphocyte\",\n",
    "    \"ductal\",\"invasive\",\"lobular\",\"metastatic\",\"biopsy\",\"stroma\",\n",
    "    \"adenocarcinoma\",\"immunohistochemistry\"\n",
    "}))\n",
    "STOPWORDS_LOW_VALUE = {\"and\",\"the\",\"with\",\"for\",\"from\",\"this\",\"that\",\"show\",\"note\",\"cell\",\"cells\",\"tissue\",\n",
    "    \"normal\",\"mild\",\"severe\",\"of\",\"to\",\"in\",\"at\",\"by\",\"is\",\"are\"}\n",
    "DO_NOT_TOUCH = {\"er\",\"pr\",\"her2\",\"her2neu\",\"ki-67\",\"ki67\",\"mm\",\"cm\",\"%\",\"score\",\"allred\"}\n",
    "\n",
    "def apply_rules(t: str) -> str:\n",
    "    t2 = re.sub(r\"[•·∙●]\", \".\", t)\n",
    "    t2 = re.sub(r\"[–—−]+\", \"-\", t2)\n",
    "    t2 = re.sub(r\"[×✕✖]\", \"x\", t2)\n",
    "    t2 = re.sub(r\"\\s{2,}\", \" \", t2)\n",
    "    return t2.strip()\n",
    "\n",
    "def apply_dictionary(t, mp):\n",
    "    def _case_aware(repl):\n",
    "        def _f(m):\n",
    "            src = m.group(0)\n",
    "            return repl.upper() if src.isupper() else repl.title() if src.istitle() else repl\n",
    "        return _f\n",
    "    count, changes = 0, {}\n",
    "    out = t\n",
    "    for wrong, right in sorted(mp.items(), key=lambda kv: len(kv[0]), reverse=True):\n",
    "        pat = rf\"\\b{re.escape(wrong)}\\b\"\n",
    "        if re.search(pat, out, flags=re.I):\n",
    "            out = re.sub(pat, _case_aware(right), out, flags=re.I)\n",
    "            count += 1\n",
    "            changes[wrong] = right\n",
    "    return out, count, changes\n",
    "\n",
    "def apply_fuzzy(t, vocab, cutoff=0.86, max_corr=200):\n",
    "    toks = re.findall(r\"[A-Za-z][A-Za-z\\-]{2,}\", t)\n",
    "    uniq = sorted({\n",
    "        x for x in (tok.lower() for tok in toks)\n",
    "        if x not in STOPWORDS_LOW_VALUE | DO_NOT_TOUCH and not re.fullmatch(r\"[a-z]*\\d+[a-z\\d\\-]*\", x)\n",
    "    })\n",
    "    repl, count = {}, 0\n",
    "    for tok in uniq:\n",
    "        best = difflib.get_close_matches(tok, vocab, n=1, cutoff=cutoff)\n",
    "        if best:\n",
    "            repl[tok] = best[0]\n",
    "            count += 1\n",
    "            if count >= max_corr: break\n",
    "    def _swap(m):\n",
    "        src = m.group(0); low = src.lower()\n",
    "        r = repl.get(low)\n",
    "        if not r: return src\n",
    "        return r.upper() if src.isupper() else r.title() if src.istitle() else r\n",
    "    out = re.sub(r\"\\b([A-Za-z][A-Za-z\\-]{3,})\\b\", _swap, t)\n",
    "    return out, len(repl), repl\n",
    "\n",
    "def scispacy_normalize(t: str):\n",
    "    if _NLP is None: return t, []\n",
    "    ents = []\n",
    "    try:\n",
    "        doc = _NLP(t)\n",
    "        for ent in doc.ents:\n",
    "            item = {\"text\": ent.text, \"label\": ent.label_, \"start\": ent.start_char, \"end\": ent.end_char}\n",
    "            if _LINKER is not None and getattr(ent._, \"kb_ents\", None):\n",
    "                if ent._.kb_ents:\n",
    "                    cui, score = ent._.kb_ents[0]\n",
    "                    kb = _LINKER.kb.cui_to_entity.get(cui)\n",
    "                    if kb:\n",
    "                        item.update({\"cui\": cui, \"canonical\": kb.canonical_name, \"link_score\": float(score)})\n",
    "            ents.append(item)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return t, ents\n",
    "\n",
    "def quickumls_normalize(t: str):\n",
    "    if _MATCHER is None: return t, []\n",
    "    results = []\n",
    "    try:\n",
    "        groups = _MATCHER.match(t, best_match=True, ignore_syntax=False)\n",
    "        for grp in groups:\n",
    "            m = grp[0]\n",
    "            results.append({\n",
    "                \"ngram\": m[\"ngram\"], \"term\": m[\"term\"], \"cui\": m[\"cui\"],\n",
    "                \"similarity\": float(m[\"similarity\"]), \"start\": m[\"start\"], \"end\": m[\"end\"]\n",
    "            })\n",
    "    except Exception:\n",
    "        pass\n",
    "    return t, results\n",
    "\n",
    "def seq_sim(a: str, b: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "@dataclass\n",
    "class BlockQA:\n",
    "    page: int\n",
    "    index: int\n",
    "    text_orig: str\n",
    "    text_clean: str\n",
    "    rules_corr: int = 0\n",
    "    dict_corr: int = 0\n",
    "    fuzzy_corr: int = 0\n",
    "    dict_map: Dict[str,str] = None\n",
    "    fuzzy_map: Dict[str,str] = None\n",
    "    scispacy_ents: List[Dict[str,Any]] = None\n",
    "    quickumls_hits: List[Dict[str,Any]] = None\n",
    "    sim: float = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16d3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 4 page files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pages = sorted(blocks_dir.glob(\"page_*_blocks.json\"))\n",
    "if not pages:\n",
    "    raise FileNotFoundError(f\"No page_*_blocks.json under {blocks_dir}\")\n",
    "print(f\"[INFO] Found {len(pages)} page files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10eefc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ page 001: in=8 → out=8 | rules=2, dict=0, fuzzy=4 | sim_mean=1.000 → page_001_blocks.domain.json\n",
      "✓ page 002: in=12 → out=12 | rules=2, dict=0, fuzzy=27 | sim_mean=0.998 → page_002_blocks.domain.json\n",
      "✓ page 003: in=8 → out=8 | rules=4, dict=0, fuzzy=0 | sim_mean=0.998 → page_003_blocks.domain.json\n",
      "✓ page 004: in=9 → out=9 | rules=7, dict=0, fuzzy=0 | sim_mean=0.989 → page_004_blocks.domain.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_rows = []\n",
    "per_page_summary = []\n",
    "\n",
    "for f in pages:\n",
    "    page_no = int(re.search(r\"page_(\\d+)_\", f.name).group(1))\n",
    "    blocks = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "    out_blocks = []\n",
    "    qa_items = []\n",
    "    rules_changed_cnt = 0\n",
    "    dict_total = 0\n",
    "    fuzzy_total = 0\n",
    "\n",
    "    for bi, b in enumerate(blocks):\n",
    "        t0 = (b.get(\"text\") or \"\").strip()\n",
    "        if not t0:\n",
    "            continue\n",
    "        t1 = apply_rules(t0)\n",
    "        rules_changed = int(t1 != t0)\n",
    "        rules_changed_cnt += rules_changed\n",
    "\n",
    "        t2, dict_n, dict_map = apply_dictionary(t1, BUILTIN_MAP)\n",
    "        dict_total += dict_n\n",
    "\n",
    "        t3, fuzzy_n, fuzzy_map = apply_fuzzy(t2, VOCAB, cutoff=float(fuzzy_cutoff), max_corr=int(max_corrections))\n",
    "        fuzzy_total += fuzzy_n\n",
    "\n",
    "        t4, ents = scispacy_normalize(t3)\n",
    "        t5, qhits = quickumls_normalize(t4)\n",
    "\n",
    "        nb = dict(b)\n",
    "        nb[\"text_cleaned\"] = t5\n",
    "        nb[\"norm_meta\"] = {\n",
    "            \"rules_applied\": bool(rules_changed),\n",
    "            \"dict_corrections\": dict_n,\n",
    "            \"fuzzy_corrections\": fuzzy_n,\n",
    "            \"scispacy_ents\": ents,\n",
    "            \"quickumls_hits\": qhits\n",
    "        }\n",
    "        out_blocks.append(nb)\n",
    "\n",
    "        qa_items.append(\n",
    "            BlockQA(\n",
    "                page=page_no, index=bi,\n",
    "                text_orig=t0, text_clean=t5,\n",
    "                rules_corr=rules_changed, dict_corr=dict_n, fuzzy_corr=fuzzy_n,\n",
    "                dict_map=dict_map, fuzzy_map=fuzzy_map,\n",
    "                scispacy_ents=ents, quickumls_hits=qhits,\n",
    "                sim=seq_sim(t0, t5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    out_file = (out_root / f\"{f.stem}.domain.json\")\n",
    "    out_file.write_text(json.dumps(out_blocks, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    if qa_items:\n",
    "        sim_mean = sum(x.sim for x in qa_items)/len(qa_items)\n",
    "        fuzzy_sum = sum(x.fuzzy_corr for x in qa_items)\n",
    "        dict_sum  = sum(x.dict_corr for x in qa_items)\n",
    "        ents_cnt  = sum(len(x.scispacy_ents) for x in qa_items)\n",
    "        cui_cnt   = sum(1 for x in qa_items for e in x.scispacy_ents if isinstance(e, dict) and \"cui\" in e)\n",
    "        qmatch    = sum(len(x.quickumls_hits) for x in qa_items)\n",
    "\n",
    "        per_page_summary.append({\n",
    "            \"page\": page_no,\n",
    "            \"blocks\": len(qa_items),\n",
    "            \"similarity_mean\": round(sim_mean, 4),\n",
    "            \"dict_corrections\": dict_sum,\n",
    "            \"fuzzy_corrections\": fuzzy_sum,\n",
    "            \"rules_changed\": rules_changed_cnt,\n",
    "            \"scispacy_ents\": ents_cnt,\n",
    "            \"scispacy_cuis\": cui_cnt,\n",
    "            \"quickumls_hits\": qmatch\n",
    "        })\n",
    "\n",
    "        df_block = pd.DataFrame([x.__dict__ for x in qa_items])\n",
    "        df_block.to_csv((out_root / \"_qa\" / f\"page_{page_no:03d}_cleanup_blocks.csv\"), index=False)\n",
    "\n",
    "    print(f\"✓ page {page_no:03d}: in={len(blocks)} → out={len(out_blocks)} | \"\n",
    "          f\"rules={rules_changed_cnt}, dict={dict_total}, fuzzy={fuzzy_total} | \"\n",
    "          f\"sim_mean={sim_mean if qa_items else 0:.3f} → {out_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389c3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Domain cleanup complete.\n",
      "Summary saved → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/02_cleaned/_qa_cleanup.csv\n",
      "Saved QA plot → /Users/balijepalli/Documents/GitHub/entheory-ai/notebooks/outputs/run_001/02_cleaned/_qa_cleanup_summary.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>blocks</th>\n",
       "      <th>similarity_mean</th>\n",
       "      <th>dict_corrections</th>\n",
       "      <th>fuzzy_corrections</th>\n",
       "      <th>rules_changed</th>\n",
       "      <th>scispacy_ents</th>\n",
       "      <th>scispacy_cuis</th>\n",
       "      <th>quickumls_hits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page  blocks  similarity_mean  dict_corrections  fuzzy_corrections  \\\n",
       "0     1       8           0.9998                 0                  4   \n",
       "1     2      12           0.9981                 0                 27   \n",
       "2     3       8           0.9983                 0                  0   \n",
       "3     4       9           0.9888                 0                  0   \n",
       "\n",
       "   rules_changed  scispacy_ents  scispacy_cuis  quickumls_hits  \n",
       "0              2              0              0               0  \n",
       "1              2              0              0               0  \n",
       "2              4              0              0               0  \n",
       "3              7              0              0               0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if per_page_summary:\n",
    "    df = pd.DataFrame(per_page_summary).sort_values(\"page\")\n",
    "    df.to_csv(out_root / \"_qa_cleanup.csv\", index=False)\n",
    "    print(\"\\n✅ Domain cleanup complete.\")\n",
    "    print(\"Summary saved →\", out_root / \"_qa_cleanup.csv\")\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"page\",\"blocks\",\"similarity_mean\",\"dict_corrections\",\"fuzzy_corrections\",\n",
    "                               \"rules_changed\",\"scispacy_ents\",\"scispacy_cuis\",\"quickumls_hits\"])\n",
    "    print(\"[WARN] No QA rows produced.\")\n",
    "\n",
    "if len(df):\n",
    "    fig, ax1 = plt.subplots(figsize=(6.5,3.2))\n",
    "    x = df[\"page\"].astype(str).str.zfill(3)\n",
    "    ax1.plot(x, df[\"similarity_mean\"], marker=\"o\")\n",
    "    ax1.set_ylabel(\"Similarity (0–1)\")\n",
    "    ax1.set_xlabel(\"page\")\n",
    "    ax1.set_ylim(0.90, 1.01)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.bar(x, df[\"fuzzy_corrections\"], alpha=0.30, label=\"Fuzzy Corrections\")\n",
    "    ax2.set_ylabel(\"Fuzzy Corrections\")\n",
    "\n",
    "    plt.title(\"OCR → Domain Cleanup QA Summary\")\n",
    "    fig.tight_layout()\n",
    "    figpath = out_root / \"_qa_cleanup_summary.png\"\n",
    "    plt.savefig(figpath, dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(\"Saved QA plot →\", figpath)\n",
    "\n",
    "df if 'df' in locals() else None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rxetl)",
   "language": "python",
   "name": "rxetl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
