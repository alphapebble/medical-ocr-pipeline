{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413cee8f",
   "metadata": {},
   "source": [
    "# 04 â€” JSON Extraction (Strict, Chunked)\n",
    "\n",
    "Convert cleaned blocks into strict schema JSON. Emits chunk inputs, raw LLM outputs, and validated JSON per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17a945",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "blocks_dir = \"outputs/run_001/03_llmcleaned\"\n",
    "schema_json = \"config/schema_prescription.json\"\n",
    "primary_model = \"alibayram/medgemma:latest\"\n",
    "fallback_model = \"mistral:7b\"\n",
    "chunk_size = 15\n",
    "max_retries = 3\n",
    "output_dir = \"outputs/run_001/04_extracted_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c559399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using input folder â†’ outputs/run_001/03_llmcleaned\n",
      "[INFO] Loaded schema from schema_prescription.json\n",
      "[INFO] Found 8 page files.\n",
      "\n",
      "[INFO] Page page_001_blocks.domain.llm: 8 blocks\n",
      "[WARN] alibayram/medgemma:latest attempt 1/3 failed: Expecting value: line 6787 column 17 (char 115950)\n",
      "  âœ“ chunk 1/1 ok â†’ page_001_blocks.domain.llm_valid_1.json\n",
      "\n",
      "[INFO] Page page_002_blocks.domain.llm: 12 blocks\n",
      "  âœ“ chunk 1/1 ok â†’ page_002_blocks.domain.llm_valid_1.json\n",
      "\n",
      "[INFO] Page page_003_blocks.domain.llm: 8 blocks\n",
      "  âœ“ chunk 1/1 ok â†’ page_003_blocks.domain.llm_valid_1.json\n",
      "\n",
      "[INFO] Page page_004_blocks.domain.llm: 9 blocks\n",
      "  âœ“ chunk 1/1 ok â†’ page_004_blocks.domain.llm_valid_1.json\n",
      "\n",
      "[INFO] Page page_001_blocks.domain.llm: 56 blocks\n",
      "  âœ“ chunk 1/4 ok â†’ page_001_blocks.domain.llm_valid_1.json\n",
      "  âœ“ chunk 2/4 ok â†’ page_001_blocks.domain.llm_valid_2.json\n",
      "  âœ“ chunk 3/4 ok â†’ page_001_blocks.domain.llm_valid_3.json\n",
      "  âœ“ chunk 4/4 ok â†’ page_001_blocks.domain.llm_valid_4.json\n",
      "\n",
      "[INFO] Page page_002_blocks.domain.llm: 74 blocks\n",
      "  âœ“ chunk 1/5 ok â†’ page_002_blocks.domain.llm_valid_1.json\n",
      "  âœ“ chunk 2/5 ok â†’ page_002_blocks.domain.llm_valid_2.json\n",
      "  âœ“ chunk 3/5 ok â†’ page_002_blocks.domain.llm_valid_3.json\n",
      "  âœ“ chunk 4/5 ok â†’ page_002_blocks.domain.llm_valid_4.json\n",
      "  âœ“ chunk 5/5 ok â†’ page_002_blocks.domain.llm_valid_5.json\n",
      "\n",
      "[INFO] Page page_003_blocks.domain.llm: 17 blocks\n",
      "  âœ“ chunk 1/2 ok â†’ page_003_blocks.domain.llm_valid_1.json\n",
      "  âœ“ chunk 2/2 ok â†’ page_003_blocks.domain.llm_valid_2.json\n",
      "\n",
      "[INFO] Page page_004_blocks.domain.llm: 30 blocks\n",
      "  âœ“ chunk 1/2 ok â†’ page_004_blocks.domain.llm_valid_1.json\n",
      "  âœ“ chunk 2/2 ok â†’ page_004_blocks.domain.llm_valid_2.json\n",
      "\n",
      "ðŸ©º merged â†’ outputs/run_001/04_jsonextracted/final_prescription.json (170 records)\n",
      "\n",
      "âœ… Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import re, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Iterable\n",
    "from langchain.prompts import PromptTemplate\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "except Exception:\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# ----------------------\n",
    "# PATH SETUP (auto-root)\n",
    "# ----------------------\n",
    "run_root = Path(\"outputs/run_001\")\n",
    "\n",
    "# Prefer latest stage that actually exists\n",
    "search_order = [\"03_llmcleaned\", \"02_cleaned\", \"01_blocks\"]\n",
    "blocks_dir = None\n",
    "for folder in search_order:\n",
    "    cand = run_root / folder\n",
    "    if any(cand.glob(\"page_*_blocks*.json\")):\n",
    "        blocks_dir = cand\n",
    "        print(f\"[INFO] Using input folder â†’ {cand}\")\n",
    "        break\n",
    "if not blocks_dir:\n",
    "    raise FileNotFoundError(f\"No JSON block files found under {run_root}\")\n",
    "\n",
    "out_dir = run_root / \"04_jsonextracted\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------\n",
    "# SCHEMA LOADING\n",
    "# ----------------\n",
    "schema_path = Path(schema_json).expanduser().resolve()\n",
    "if not schema_path.exists():\n",
    "    raise FileNotFoundError(schema_path)\n",
    "schema_str = schema_path.read_text(encoding=\"utf-8\")\n",
    "try:\n",
    "    schema = json.loads(schema_str)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Schema must be JSON. Could not parse {schema_path}: {e}\")\n",
    "print(f\"[INFO] Loaded schema from {schema_path.name}\")\n",
    "\n",
    "# ----------------\n",
    "# FILE DISCOVERY\n",
    "# ----------------\n",
    "def normalize_page_id(name: str) -> str:\n",
    "    # page_1_blocks â†’ page_001_blocks (helps consistency if mixed sources exist)\n",
    "    return re.sub(r\"(page_)(\\d{1,3})(_)\", lambda m: f\"{m.group(1)}{int(m.group(2)):03d}{m.group(3)}\", name)\n",
    "\n",
    "page_files = sorted(blocks_dir.glob(\"page_*_blocks*.json\"))\n",
    "if not page_files:\n",
    "    raise FileNotFoundError(f\"No page_* JSON files found in {blocks_dir}\")\n",
    "print(f\"[INFO] Found {len(page_files)} page files.\")\n",
    "\n",
    "# -------------\n",
    "# LLM PLUMBING\n",
    "# -------------\n",
    "fmt = StrOutputParser()\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You convert OCR text blocks into a single JSON object that STRICTLY conforms to the schema below.\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \" - Output ONLY raw JSON. No prose, no markdown fences, no comments.\\n\"\n",
    "    \" - Do NOT invent facts. If a field is missing, leave it empty or null as per the schema.\\n\"\n",
    "    \" - Preserve medical wording, numbers, signs, and units.\\n\"\n",
    "    \" - Use the input blocks' order and content faithfully.\\n\\n\"\n",
    "    \"Schema:\\n{schema}\\n\\n\"\n",
    "    \"Blocks (list of objects with bbox & text):\\n{blocks}\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(SYSTEM_PROMPT)\n",
    "\n",
    "def make_llm(name: str):\n",
    "    try:\n",
    "        # `format=json` nudges some Ollama models to keep JSON\n",
    "        return ChatOllama(model=name, temperature=0,\n",
    "                          model_kwargs={\"keep_alive\": 0, \"format\": \"json\"})\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not init LLM\", name, e)\n",
    "        return None\n",
    "\n",
    "def strip_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```(?:json)?\\s*\", \"\", s)\n",
    "        s = re.sub(r\"\\s*```$\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def light_repair(s: str) -> str:\n",
    "    # Common tiny repairs (still safe). We do NOT try to invent keys.\n",
    "    s = strip_fences(s)\n",
    "    # Remove trailing commas before ] or }\n",
    "    s = re.sub(r\",(\\s*[\\]\\}])\", r\"\\1\", s)\n",
    "    # Ensure it starts with { or [\n",
    "    m = re.search(r\"[\\{\\[]\", s)\n",
    "    if m and m.start() > 0:\n",
    "        s = s[m.start():]\n",
    "    return s\n",
    "\n",
    "def parse_json_maybe(s: str):\n",
    "    s1 = strip_fences(s)\n",
    "    try:\n",
    "        return json.loads(s1)\n",
    "    except Exception:\n",
    "        s2 = light_repair(s1)\n",
    "        return json.loads(s2)  # will raise if still invalid\n",
    "\n",
    "# -----------------\n",
    "# TEXT SELECTION\n",
    "# -----------------\n",
    "def best_text(b: Dict[str, Any]) -> str:\n",
    "    # Priority: LLM â†’ domain cleaned â†’ raw\n",
    "    for k in (\"text_llm\", \"text_cleaned\", \"text\"):\n",
    "        v = b.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v\n",
    "    return \"\"\n",
    "\n",
    "def blocks_payload(chunk: List[Dict[str, Any]]) -> str:\n",
    "    slim = []\n",
    "    for b in chunk:\n",
    "        slim.append({\n",
    "            \"bbox\": b.get(\"bbox\", [0, 0, 1, 1]),\n",
    "            \"text\": best_text(b),\n",
    "            \"source\": b.get(\"source\", \"\")\n",
    "        })\n",
    "    return json.dumps(slim, ensure_ascii=False)\n",
    "\n",
    "# ---------------\n",
    "# CHUNKING UTILS\n",
    "# ---------------\n",
    "def chunks_by_size(items: List[Dict[str, Any]], max_chars: int) -> Iterable[List[Dict[str, Any]]]:\n",
    "    \"\"\"Chunk list so that json.dumps(list) stays under ~max_chars.\"\"\"\n",
    "    current, size = [], 0\n",
    "    for it in items:\n",
    "        t = best_text(it)\n",
    "        # estimate addition size (text + bbox + overhead)\n",
    "        est = len(t) + 180\n",
    "        if current and size + est > max_chars:\n",
    "            yield current\n",
    "            current, size = [], 0\n",
    "        current.append(it)\n",
    "        size += est\n",
    "    if current:\n",
    "        yield current\n",
    "\n",
    "# --------------\n",
    "# CORE INFERENCE\n",
    "# --------------\n",
    "def try_model(model_name: str, blocks_json: str, max_retries: int):\n",
    "    llm = make_llm(model_name)\n",
    "    if not llm:\n",
    "        return None\n",
    "    chain = prompt | llm | fmt\n",
    "\n",
    "    payload = blocks_json\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            out = chain.invoke({\"schema\": schema_str, \"blocks\": payload})\n",
    "            obj = parse_json_maybe(out)\n",
    "            return obj\n",
    "        except Exception as e:\n",
    "            # progressive shrink: trim characters from payload text\n",
    "            print(f\"[WARN] {model_name} attempt {attempt+1}/{max_retries} failed: {e}\")\n",
    "            try:\n",
    "                # shrink by 20% characters\n",
    "                cut = max(1, int(len(payload) * 0.8))\n",
    "                payload = payload[:cut]\n",
    "                # try to end on a JSON boundary if possible\n",
    "                payload = re.sub(r\",\\s*?\\Z\", \"\", payload)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "# ----------------\n",
    "# MAIN (per page)\n",
    "# ----------------\n",
    "piece_paths: List[str] = []\n",
    "primary = primary_model\n",
    "fallback = fallback_model if 'fallback_model' in globals() else None\n",
    "max_retries = int(max_retries)\n",
    "\n",
    "# If user provided chunk_size (count), we still respect a char budget to avoid overflows.\n",
    "# Default char budget is ~90k chars which is safe for many local models.\n",
    "char_budget = int(globals().get(\"char_budget\", 90000))\n",
    "\n",
    "for pf in page_files:\n",
    "    # Keep pages separate for locality/coherence\n",
    "    try:\n",
    "        blocks = json.loads(pf.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {pf.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    page_id = normalize_page_id(pf.stem)  # for consistent filenames/logs\n",
    "    total = len(blocks)\n",
    "    print(f\"\\n[INFO] Page {page_id}: {total} blocks\")\n",
    "\n",
    "    # Chunk this single page by char budget (or by user chunk_size if smaller)\n",
    "    max_count = int(globals().get(\"chunk_size\", 999999))\n",
    "    page_chunks: List[List[Dict[str, Any]]] = []\n",
    "    # First generate by char budget, then split any oversized chunk by count\n",
    "    for ch in chunks_by_size(blocks, char_budget):\n",
    "        if len(ch) <= max_count:\n",
    "            page_chunks.append(ch)\n",
    "        else:\n",
    "            # split by count\n",
    "            for i in range(0, len(ch), max_count):\n",
    "                page_chunks.append(ch[i:i+max_count])\n",
    "\n",
    "    for ci, chunk in enumerate(page_chunks, start=1):\n",
    "        payload = blocks_payload(chunk)\n",
    "        # Trace input\n",
    "        (out_dir / f\"{page_id}_input_{ci}.json\").write_text(payload, encoding=\"utf-8\")\n",
    "\n",
    "        parsed = try_model(primary, payload, max_retries)\n",
    "        if parsed is None and fallback:\n",
    "            print(f\"[INFO] Fallback model on {page_id} chunk {ci} â†’ {fallback}\")\n",
    "            parsed = try_model(fallback, payload, max_retries)\n",
    "\n",
    "        if parsed is not None:\n",
    "            vp = out_dir / f\"{page_id}_valid_{ci}.json\"\n",
    "            vp.write_text(json.dumps(parsed, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "            piece_paths.append(str(vp))\n",
    "            print(f\"  âœ“ chunk {ci}/{len(page_chunks)} ok â†’ {vp.name}\")\n",
    "        else:\n",
    "            print(f\"  âœ— chunk {ci}/{len(page_chunks)} failed\")\n",
    "\n",
    "# -------------\n",
    "# MERGE OUTPUT\n",
    "# -------------\n",
    "if piece_paths:\n",
    "    merged: List[Any] = []\n",
    "    for p in piece_paths:\n",
    "        try:\n",
    "            merged.extend(json.loads(Path(p).read_text(encoding=\"utf-8\")))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {Path(p).name}: {e}\")\n",
    "    final_path = out_dir / \"final_prescription.json\"\n",
    "    final_path.write_text(json.dumps(merged, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    print(f\"\\nðŸ©º merged â†’ {final_path} ({len(merged)} records)\")\n",
    "else:\n",
    "    print(\"\\n[WARN] No valid chunks to merge.\")\n",
    "\n",
    "print(\"\\nâœ… Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1800516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[QA] Verifying input files in outputs/run_001/03_llmcleaned...\n",
      "  âœ“ page_001_blocks.domain.llm.json: 8 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_002_blocks.domain.llm.json: 12 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_003_blocks.domain.llm.json: 8 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_004_blocks.domain.llm.json: 9 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_1_blocks.domain.llm.json: 56 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_2_blocks.domain.llm.json: 74 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_3_blocks.domain.llm.json: 17 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n",
      "  âœ“ page_4_blocks.domain.llm.json: 30 blocks, sample keys=['bbox', 'text', 'source', 'confidence', 'section']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def verify_input_jsons(input_dir: Path):\n",
    "    print(f\"\\n[QA] Verifying input files in {input_dir}...\")\n",
    "    for f in sorted(input_dir.glob(\"page_*_blocks*.json\")):\n",
    "        try:\n",
    "            data = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            if not isinstance(data, list) or not data:\n",
    "                print(f\"[WARN] {f.name}: empty or non-list\")\n",
    "                continue\n",
    "            if not all(isinstance(x, dict) for x in data):\n",
    "                print(f\"[WARN] {f.name}: malformed entries (not dicts)\")\n",
    "                continue\n",
    "            sample = data[0]\n",
    "            keys = list(sample.keys())[:5]\n",
    "            print(f\"  âœ“ {f.name}: {len(data)} blocks, sample keys={keys}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f.name}: {e}\")\n",
    "\n",
    "# Verify input files\n",
    "verify_input_jsons(blocks_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0539eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[QA] Verifying chunked valid JSONs in outputs/run_001/04_jsonextracted...\n",
      "[WARN] page_001_blocks.domain.llm_valid_1.json: empty or non-list\n",
      "[WARN] page_001_blocks.domain.llm_valid_2.json: empty or non-list\n",
      "[WARN] page_001_blocks.domain.llm_valid_3.json: empty or non-list\n",
      "[WARN] page_001_blocks.domain.llm_valid_4.json: empty or non-list\n",
      "[WARN] page_002_blocks.domain.llm_valid_1.json: empty or non-list\n",
      "[WARN] page_002_blocks.domain.llm_valid_2.json: empty or non-list\n",
      "[WARN] page_002_blocks.domain.llm_valid_3.json: empty or non-list\n",
      "[WARN] page_002_blocks.domain.llm_valid_4.json: empty or non-list\n",
      "[WARN] page_002_blocks.domain.llm_valid_5.json: empty or non-list\n",
      "[WARN] page_003_blocks.domain.llm_valid_1.json: empty or non-list\n",
      "[WARN] page_003_blocks.domain.llm_valid_2.json: empty or non-list\n",
      "[WARN] page_004_blocks.domain.llm_valid_1.json: empty or non-list\n",
      "[WARN] page_004_blocks.domain.llm_valid_2.json: empty or non-list\n"
     ]
    }
   ],
   "source": [
    "def verify_valid_jsons(valid_dir: Path):\n",
    "    print(f\"\\n[QA] Verifying chunked valid JSONs in {valid_dir}...\")\n",
    "    for f in sorted(valid_dir.glob(\"*_valid_*.json\")):\n",
    "        try:\n",
    "            data = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            if not isinstance(data, list) or not data:\n",
    "                print(f\"[WARN] {f.name}: empty or non-list\")\n",
    "                continue\n",
    "            empty_count = sum(1 for x in data if not any(v for v in x.values()))\n",
    "            print(f\"  âœ“ {f.name}: {len(data)} records ({empty_count} empty)\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f.name}: {e}\")\n",
    "# Verify valid chunked files\n",
    "verify_valid_jsons(out_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf77d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY] Total valid chunks: 13\n",
      "[SUMMARY] Total merged records (pre-merge): 130\n"
     ]
    }
   ],
   "source": [
    "def summarize_extraction(valid_dir: Path):\n",
    "    valid_files = sorted(valid_dir.glob(\"*_valid_*.json\"))\n",
    "    total_records = 0\n",
    "    for f in valid_files:\n",
    "        data = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "        total_records += len(data)\n",
    "    print(f\"\\n[SUMMARY] Total valid chunks: {len(valid_files)}\")\n",
    "    print(f\"[SUMMARY] Total merged records (pre-merge): {total_records}\")\n",
    "\n",
    "summarize_extraction(out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rxetl)",
   "language": "python",
   "name": "rxetl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
