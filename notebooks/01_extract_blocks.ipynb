{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95308c8",
   "metadata": {},
   "source": [
    "# 01 — Extract Blocks (Native + OCR)\n",
    "\n",
    "From PDF to page-level native, OCR, and merged blocks. Emits per-page JSON artifacts.\n",
    "**Papermill parameters:** `pdf_path`, `ocr_lang`, `merge_strategy`, `output_dir`, `dpi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312f1d6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "pdf_path = \"input_pdfs/ET1-Adobe Scan 10 Sept 2025.pdf\"\n",
    "output_dir = \"outputs/run_001/01_blocks\"\n",
    "ocr_lang = \"en\"\n",
    "merge_strategy = \"hybrid\"\n",
    "dpi = 300\n",
    "\n",
    "# --- OCR / Language ---\n",
    "ocr_lang        = \"en\"         # 'en','hi','te','mr','ta' or mix of above\n",
    "dpi             = 300          # rasterization dpi for OCR\n",
    "use_easyocr     = True\n",
    "use_tesseract   = True\n",
    "use_trocr       = False\n",
    "use_paddle      = True         \n",
    "use_surya       = True\n",
    "\n",
    "\n",
    "# Paddle tuning (safe defaults)\n",
    "paddle_det_model    = \"DB\"      # detector\n",
    "paddle_rec_model    = \"SVTR_LCNet\" # recognizer (good for Latin+Indic)\n",
    "paddle_use_anglecls = True\n",
    "paddle_gpu          = False     # True if you’ve set up CUDA for Paddle\n",
    "\n",
    "# --- Preprocessing ---\n",
    "mask_banners    = True         # neutralize big colored headers/footers before OCR\n",
    "banner_top_pct  = 0.18         # top colored band height fraction\n",
    "banner_bot_pct  = 0.20         # bottom colored band height fraction\n",
    "\n",
    "# --- Adaptive OCR knobs ---\n",
    "FAST_DPI        = 200\n",
    "FAST_MAX_SIDE   = 1800\n",
    "HEAVY_DPI       = 300\n",
    "HEAVY_MAX_SIDE  = 3000\n",
    "\n",
    "NATIVE_STRONG_CHARS = 200   # if native >= this, skip heavy OCR\n",
    "FAST_MIN_CHARS      = 400   # if fast OCR < this → escalate\n",
    "FAST_MIN_MEANCONF   = 0.70\n",
    "FAST_MIN_LINES      = 10\n",
    "\n",
    "GRID_RESCUE         = True  # try heavy OCR on sparse cells\n",
    "GRID_N              = 3     # 3x3\n",
    "CELL_MIN_CHARS      = 60    # if a cell has < this, try heavy on that cell\n",
    "\n",
    "# --- Merge / Filtering ---\n",
    "native_len_gate = 100          # if native chars < this, prefer OCR\n",
    "min_conf        = 0.50         # drop OCR fragments below this conf (unless no alternative)\n",
    "line_join_px    = 14           # y-gap threshold (screen px) to join into lines\n",
    "para_join_px    = 26           # y-gap threshold to join lines into paragraphs\n",
    "dedup_iou_thr   = 0.50         # bbox IoU threshold to consider same region\n",
    "dedup_sim_thr   = 0.92         # text similarity threshold (Levenshtein) to dedup\n",
    "\n",
    "# --- Visualization ---\n",
    "make_viz_png    = True         # dump quick overlay PNGs (for debugging)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def clamp_long_side(pil: Image.Image, max_side:int) -> Image.Image:\n",
    "    w, h = pil.size\n",
    "    s = max(w, h)\n",
    "    if s <= max_side:\n",
    "        return pil\n",
    "    scale = max_side / s\n",
    "    return pil.resize((int(w*scale), int(h*scale)))\n",
    "\n",
    "def page_metrics(blocks):\n",
    "    if not blocks: \n",
    "        return 0, 0.0, 0\n",
    "    chars = sum(len(b.get(\"text\",\"\")) for b in blocks)\n",
    "    confs = [float(b.get(\"confidence\", 0.0)) for b in blocks if \"confidence\" in b]\n",
    "    mean_conf = (sum(confs)/len(confs)) if confs else 0.0\n",
    "    return chars, mean_conf, len(blocks)\n",
    "\n",
    "def need_escalation(native_chars:int, fast_chars:int, fast_mean_conf:float, fast_lines:int)->bool:\n",
    "    if native_chars >= NATIVE_STRONG_CHARS:\n",
    "        return False\n",
    "    return (fast_chars < FAST_MIN_CHARS) or (fast_mean_conf < FAST_MIN_MEANCONF) or (fast_lines < FAST_MIN_LINES)\n",
    "\n",
    "def split_grid(pil: Image.Image, n:int):\n",
    "    w, h = pil.size\n",
    "    cw, ch = w//n, h//n\n",
    "    cells=[]\n",
    "    for gy in range(n):\n",
    "        for gx in range(n):\n",
    "            x0, y0 = gx*cw, gy*ch\n",
    "            x1, y1 = (gx+1)*cw if gx<n-1 else w, (gy+1)*ch if gy<n-1 else h\n",
    "            cells.append(((x0,y0,x1,y1), pil.crop((x0,y0,x1,y1))))\n",
    "    return cells\n",
    "\n",
    "def blocks_in_cell(blocks, x0,y0,x1,y1):\n",
    "    out=[]\n",
    "    for b in blocks:\n",
    "        bx0,by0,bx1,by1 = b[\"bbox\"]\n",
    "        if bx1<=x0 or by1<=y0 or bx0>=x1 or by0>=y1: \n",
    "            continue\n",
    "        out.append(b)\n",
    "    return out\n",
    "\n",
    "def translate_blocks(blocks, dx, dy, source_suffix=None):\n",
    "    out=[]\n",
    "    for b in blocks:\n",
    "        nb = dict(b)\n",
    "        x0,y0,x1,y1 = b[\"bbox\"]\n",
    "        nb[\"bbox\"] = [x0+dx, y0+dy, x1+dx, y1+dy]\n",
    "        if source_suffix:\n",
    "            nb[\"source\"] = f\"{b.get('source','')}{source_suffix}\"\n",
    "        out.append(nb)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_paddle_fast = None\n",
    "_paddle_heavy = None\n",
    "\n",
    "def build_paddle(langs, heavy=False):\n",
    "    \"\"\"Version-safe init; tries to pick mobile for fast and server for heavy (if supported).\"\"\"\n",
    "    from paddleocr import PaddleOCR\n",
    "    import inspect\n",
    "    supported = {\"en\",\"hi\",\"te\",\"mr\",\"ta\"}\n",
    "    paddle_lang = next((l for l in langs if l in supported), \"en\")\n",
    "\n",
    "    params = set(inspect.signature(PaddleOCR).parameters.keys())\n",
    "    kw = {\"lang\": paddle_lang}\n",
    "    # Angle classifier if present\n",
    "    if \"use_angle_cls\" in params:\n",
    "        kw[\"use_angle_cls\"] = True\n",
    "    # New API doc flags (3.x)\n",
    "    if \"use_doc_orientation_classify\" in params:\n",
    "        kw.update(\n",
    "            use_doc_orientation_classify=False,\n",
    "            use_doc_unwarping=False,\n",
    "            use_textline_orientation=False,\n",
    "        )\n",
    "    # Prefer mobile on fast, server on heavy if your version exposes model selectors\n",
    "    if \"det_model\" in params:\n",
    "        kw[\"det_model\"] = \"PP-OCRv5_server_det\" if heavy else \"PP-OCRv5_mobile_det\"\n",
    "    if \"rec_model\" in params:\n",
    "        rec_prefix = paddle_lang + \"_\" if paddle_lang != \"en\" else \"en_\"\n",
    "        kw[\"rec_model\"] = f\"{rec_prefix}PP-OCRv5_mobile_rec\"  # mobile rec is fine both\n",
    "\n",
    "    return PaddleOCR(**kw)\n",
    "\n",
    "def get_paddle_fast(langs):\n",
    "    global _paddle_fast\n",
    "    if _paddle_fast is None:\n",
    "        try: _paddle_fast = build_paddle(langs, heavy=False)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Paddle FAST init failed:\", e); _paddle_fast = None\n",
    "    return _paddle_fast\n",
    "\n",
    "def get_paddle_heavy(langs):\n",
    "    global _paddle_heavy\n",
    "    if _paddle_heavy is None:\n",
    "        try: _paddle_heavy = build_paddle(langs, heavy=True)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Paddle HEAVY init failed, falling back to FAST:\", e); _paddle_heavy = get_paddle_fast(langs)\n",
    "    return _paddle_heavy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1) Imports & Setup\n",
    "\n",
    "# %%\n",
    "import io, os, json, re, math, traceback\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Core\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image, ImageDraw\n",
    "import easyocr\n",
    "import pytesseract\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"'pin_memory' argument\")\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "# Optional TrOCR (handwriting)\n",
    "trocr_processor = trocr_model = None\n",
    "if use_trocr:\n",
    "    try:\n",
    "        trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "        trocr_model     = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "        trocr_model.eval()\n",
    "        print(\"[INFO] TrOCR initialized.\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] TrOCR init failed, disabling handwriting OCR:\", e)\n",
    "        trocr_processor = trocr_model = None\n",
    "        use_trocr = False\n",
    "\n",
    "# Fuzzy similarity (for dedup)\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "except Exception:\n",
    "    def fuzz_ratio(a,b):  # tiny fallback\n",
    "        try:\n",
    "            from difflib import SequenceMatcher\n",
    "            return int(100*SequenceMatcher(None, a, b).ratio())\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "# Paths\n",
    "run_root = output_dir\n",
    "pdf_path = Path(pdf_path).expanduser().resolve()\n",
    "out_dir  = Path(run_root).expanduser().resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[INFO] PDF:\", pdf_path)\n",
    "print(\"[INFO] Out:\", out_dir)\n",
    "\n",
    "# Language map\n",
    "LANG_MAP = {\n",
    "    \"en\": [\"en\"],\n",
    "    \"hi\": [\"hi\",\"en\"],\n",
    "    \"te\": [\"te\",\"en\"],\n",
    "    \"mr\": [\"mr\",\"en\"],\n",
    "    \"ta\": [\"ta\",\"en\"]\n",
    "}\n",
    "langs = LANG_MAP.get(ocr_lang.lower(), [\"en\"])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2) Utilities\n",
    "\n",
    "# %%\n",
    "def page_to_image(doc, page_index: int, dpi: int=300) -> Image.Image:\n",
    "    \"\"\"Rasterize a PDF page to PIL image.\"\"\"\n",
    "    page = doc[page_index]\n",
    "    zoom = dpi / 72\n",
    "    mat  = fitz.Matrix(zoom, zoom)\n",
    "    pix  = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    return Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "def mask_bands(pil: Image.Image, top_pct: float, bot_pct: float) -> Image.Image:\n",
    "    \"\"\"Roughly paint over colored banners to reduce junk OCR.\"\"\"\n",
    "    if not mask_banners: \n",
    "        return pil\n",
    "    w,h   = pil.size\n",
    "    top_h = int(h * max(0, min(0.45, top_pct)))\n",
    "    bot_h = int(h * max(0, min(0.45, bot_pct)))\n",
    "    out   = pil.copy()\n",
    "    draw  = ImageDraw.Draw(out)\n",
    "    # estimate background\n",
    "    bg = (240,240,240)\n",
    "    if top_h>0: draw.rectangle([0,0,w,top_h], fill=bg)\n",
    "    if bot_h>0: draw.rectangle([0,h-bot_h,w,h], fill=bg)\n",
    "    return out\n",
    "\n",
    "def blocks_sort_key(b):\n",
    "    y0 = round(b[\"bbox\"][1],1)\n",
    "    x0 = round(b[\"bbox\"][0],1)\n",
    "    return (y0, x0)\n",
    "\n",
    "def iou(a, b) -> float:\n",
    "    ax0, ay0, ax1, ay1 = a\n",
    "    bx0, by0, bx1, by1 = b\n",
    "    inter_x0 = max(ax0, bx0); inter_y0 = max(ay0, by0)\n",
    "    inter_x1 = min(ax1, bx1); inter_y1 = min(ay1, by1)\n",
    "    iw = max(0.0, inter_x1 - inter_x0); ih = max(0.0, inter_y1 - inter_y0)\n",
    "    inter = iw * ih\n",
    "    if inter <= 0: return 0.0\n",
    "    area_a = (ax1-ax0)*(ay1-ay0); area_b = (bx1-bx0)*(by1-by0)\n",
    "    return inter / max(1e-6, area_a + area_b - inter)\n",
    "\n",
    "def section_of(bbox, page_h):\n",
    "    cy = 0.5 * (bbox[1] + bbox[3])\n",
    "    if cy < 0.25*page_h: return \"header\"\n",
    "    if cy > 0.85*page_h: return \"footer\"\n",
    "    return \"body\"\n",
    "\n",
    "\n",
    "# --- Lazy singletons for OCR engines ---\n",
    "_easy_reader = None\n",
    "_paddle_ocr  = None\n",
    "\n",
    "def get_easyocr(langs):\n",
    "    global _easy_reader\n",
    "    if _easy_reader is None:\n",
    "        _easy_reader = easyocr.Reader(langs, gpu=False)\n",
    "    return _easy_reader\n",
    "\n",
    "def _map_langs_to_paddle(langs):\n",
    "    \"\"\"\n",
    "    Map your LANG_MAP entries to PaddleOCR lang tags.\n",
    "    Paddle supports: 'en','hi','te','mr','ta' (and many more).\n",
    "    We'll pick the first supported language; Paddle is single-lang per OCR instance.\n",
    "    \"\"\"\n",
    "    supported = {\"en\",\"hi\",\"te\",\"mr\",\"ta\"}\n",
    "    for l in langs:\n",
    "        if l in supported:\n",
    "            return l\n",
    "    return \"en\"\n",
    "def get_paddleocr(langs):\n",
    "    \"\"\"Works with PaddleOCR 2.7 → 3.x (no deprecated kwargs).\"\"\"\n",
    "    global _paddle_ocr\n",
    "    if _paddle_ocr is not None:\n",
    "        return _paddle_ocr\n",
    "\n",
    "    try:\n",
    "        from paddleocr import PaddleOCR\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] PaddleOCR import failed; disabling:\", e)\n",
    "        return None\n",
    "\n",
    "    supported = {\"en\",\"hi\",\"te\",\"mr\",\"ta\"}\n",
    "    paddle_lang = next((l for l in langs if l in supported), \"en\")\n",
    "\n",
    "    # Build kwargs by introspecting supported params\n",
    "    import inspect\n",
    "    sig = inspect.signature(PaddleOCR)\n",
    "    params = sig.parameters\n",
    "    init_kwargs = {\"lang\": paddle_lang}\n",
    "    if \"use_angle_cls\" in params:              # old & some mid builds\n",
    "        init_kwargs[\"use_angle_cls\"] = True\n",
    "    if \"use_doc_orientation_classify\" in params:  # new 3.x builds\n",
    "        init_kwargs.update(\n",
    "            use_doc_orientation_classify=False,\n",
    "            use_doc_unwarping=False,\n",
    "            use_textline_orientation=False,\n",
    "        )\n",
    "    # DO NOT pass: show_log / use_gpu / det_algorithm / rec_algorithm\n",
    "\n",
    "    try:\n",
    "        _paddle_ocr = PaddleOCR(**init_kwargs)\n",
    "        print(f\"[INFO] PaddleOCR initialized (lang={paddle_lang})\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] PaddleOCR init failed:\", e)\n",
    "        _paddle_ocr = None\n",
    "    return _paddle_ocr\n",
    "\n",
    "\n",
    "\n",
    "#-- surya ---\n",
    "\n",
    "_surya = None\n",
    "\n",
    "def get_surya():\n",
    "    global _surya\n",
    "    if _surya is None:\n",
    "        from surya.ocr import Reader  # API wrapper; thin, loads detection/rec/LA models\n",
    "        _surya = Reader()             # Surya auto-handles multilingual and layout\n",
    "        print(\"[INFO] Surya OCR initialized\")\n",
    "    return _surya\n",
    "\n",
    "def extract_surya(pil: Image.Image) -> list[dict]:\n",
    "    if not use_surya:\n",
    "        return []\n",
    "    try:\n",
    "        reader = get_surya()\n",
    "        # Surya accepts numpy arrays; returns lines with boxes & confidence\n",
    "        arr = np.array(pil)\n",
    "        result = reader.read(arr, return_lines=True)  # lines with bbox + text + conf\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Surya failed:\", e); return []\n",
    "    out = []\n",
    "    for ln in result or []:\n",
    "        # ln: {\"text\":..., \"confidence\":..., \"bbox\":[x0,y0,x1,y1]}  (API shape simplified)\n",
    "        txt = (ln.get(\"text\") or \"\").strip()\n",
    "        if not txt: continue\n",
    "        conf = float(ln.get(\"confidence\", 0.0))\n",
    "        if conf < min_conf: continue\n",
    "        x0,y0,x1,y1 = [float(v) for v in ln.get(\"bbox\", [0,0,0,0])]\n",
    "        out.append({\"bbox\":[x0,y0,x1,y1], \"text\":txt, \"confidence\":conf, \"source\":\"surya\"})\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "# --- end surya ---\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3) Extractors\n",
    "\n",
    "# %%\n",
    "def extract_native(page) -> List[Dict[str,Any]]:\n",
    "    \"\"\"PyMuPDF block extraction.\"\"\"\n",
    "    out = []\n",
    "    for b in page.get_text(\"blocks\"):\n",
    "        if len(b) >= 5:\n",
    "            x0,y0,x1,y1,txt = b[:5]\n",
    "            if txt and str(txt).strip():\n",
    "                out.append({\n",
    "                    \"bbox\":[float(x0),float(y0),float(x1),float(y1)],\n",
    "                    \"text\":str(txt).strip(),\n",
    "                    \"source\":\"native\",\n",
    "                    \"confidence\":1.0\n",
    "                })\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "def extract_easyocr(pil: Image.Image, langs: List[str]) -> List[Dict[str,Any]]:\n",
    "    if not easyocr or not use_easyocr:\n",
    "        return []\n",
    "    # reader = easyocr.Reader(langs, gpu=False)\n",
    "    reader = get_easyocr(langs)   \n",
    "    res = reader.readtext(np.array(pil), detail=1, paragraph=True)\n",
    "    out=[]\n",
    "    for item in res:\n",
    "        if not isinstance(item,(list,tuple)) or len(item)<2: \n",
    "            continue\n",
    "        bbox, text = item[0], item[1]\n",
    "        conf = float(item[2]) if len(item)>2 else 1.0\n",
    "        try:\n",
    "            xs=[p[0] for p in bbox]; ys=[p[1] for p in bbox]\n",
    "            x0,y0,x1,y1 = min(xs),min(ys),max(xs),max(ys)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if conf < min_conf: \n",
    "            continue\n",
    "        out.append({\"bbox\":[x0,y0,x1,y1],\"text\":(text or \"\").strip(),\"confidence\":conf,\"source\":\"easyocr\"})\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "def extract_paddleocr(ocr, pil: Image.Image) -> list[dict]:\n",
    "    if ocr is None:\n",
    "        return []\n",
    "    tmp = out_dir / \"_paddle_tmp.png\"\n",
    "    pil.save(tmp)\n",
    "    try:\n",
    "        result = ocr.predict(str(tmp)) if hasattr(ocr,\"predict\") else ocr.ocr(str(tmp))\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Paddle predict/ocr failed:\", e); result=None\n",
    "    finally:\n",
    "        try: tmp.unlink(missing_ok=True)\n",
    "        except: pass\n",
    "    if not result:\n",
    "        return []\n",
    "    # robust parse\n",
    "    out=[]\n",
    "    batches = result if isinstance(result, list) else [result]\n",
    "    for batch in batches or []:\n",
    "        for it in batch or []:\n",
    "            try:\n",
    "                if isinstance(it, dict):\n",
    "                    txt=(it.get(\"text\") or \"\").strip()\n",
    "                    conf=float(it.get(\"score\",it.get(\"confidence\",0.0)))\n",
    "                    box=it.get(\"box\") or it.get(\"bbox\")\n",
    "                    if not (txt and box): continue\n",
    "                    xs=[p[0] for p in box]; ys=[p[1] for p in box]\n",
    "                else:\n",
    "                    poly, rec = it[0], it[1]\n",
    "                    txt=(rec[0] or \"\").strip()\n",
    "                    conf=float(rec[1] or 0.0)\n",
    "                    xs=[p[0] for p in poly]; ys=[p[1] for p in poly]\n",
    "                if conf < min_conf or not txt: continue\n",
    "                out.append({\"bbox\":[min(xs),min(ys),max(xs),max(ys)],\n",
    "                            \"text\":txt,\"confidence\":conf,\"source\":\"paddle\"})\n",
    "            except: \n",
    "                continue\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "\n",
    "def extract_tesseract(pil: Image.Image, langs: List[str]) -> List[Dict[str,Any]]:\n",
    "    if not pytesseract or not use_tesseract:\n",
    "        return []\n",
    "    lang = \"+\".join({\n",
    "        \"en\":\"eng\",\"hi\":\"hin\",\"te\":\"tel\",\"mr\":\"mar\",\"ta\":\"tam\"\n",
    "    }.get(x,\"eng\") for x in langs)\n",
    "    cfg = \"--oem 3 --psm 6\"\n",
    "    txt = pytesseract.image_to_string(pil, lang=lang, config=cfg)\n",
    "    if not txt.strip(): \n",
    "        return []\n",
    "    # one big block (tess loses layout here); we still keep it to not miss content\n",
    "    w,h = pil.size\n",
    "    return [{\"bbox\":[0,0,w,h], \"text\":txt.strip(), \"confidence\":0.8, \"source\":\"tesseract\"}]\n",
    "\n",
    "def extract_trocr(pil: Image.Image) -> List[Dict[str,Any]]:\n",
    "    \"\"\"Very rough line-based handwriting OCR with TrOCR (slow).\"\"\"\n",
    "    if not use_trocr or not (trocr_model and trocr_processor):\n",
    "        return []\n",
    "    # naive: resize & run as single image\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        pixel_values = trocr_processor(images=pil, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = trocr_model.generate(pixel_values)\n",
    "        text = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    w,h = pil.size\n",
    "    return [{\"bbox\":[0,0,w,h], \"text\":text.strip(), \"confidence\":0.75, \"source\":\"trocr\"}]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4) Post–processing (regroup, dedup, merge)\n",
    "\n",
    "# %%\n",
    "def regroup_lines(blocks: List[Dict[str,Any]], line_gap:int=14, para_gap:int=26)->List[Dict[str,Any]]:\n",
    "    \"\"\"Join tiny fragments into lines & paragraphs by Y proximity and reading order.\"\"\"\n",
    "    if not blocks: \n",
    "        return []\n",
    "    # sort by y,x\n",
    "    bs = sorted(blocks, key=blocks_sort_key)\n",
    "    rows = []\n",
    "    cur = [bs[0]]\n",
    "    for b in bs[1:]:\n",
    "        prev = cur[-1]\n",
    "        # if near same line\n",
    "        if abs(b[\"bbox\"][1] - prev[\"bbox\"][1]) <= line_gap:\n",
    "            cur.append(b)\n",
    "        else:\n",
    "            rows.append(cur); cur=[b]\n",
    "    rows.append(cur)\n",
    "\n",
    "    # join line fragments\n",
    "    lines=[]\n",
    "    for row in rows:\n",
    "        row = sorted(row, key=lambda x:x[\"bbox\"][0])\n",
    "        text = \" \".join(x[\"text\"] for x in row if x[\"text\"])\n",
    "        x0 = min(x[\"bbox\"][0] for x in row); y0=min(x[\"bbox\"][1] for x in row)\n",
    "        x1 = max(x[\"bbox\"][2] for x in row); y1=max(x[\"bbox\"][3] for x in row)\n",
    "        src= \"+\".join(sorted(set(x[\"source\"] for x in row)))\n",
    "        conf=sum(x.get(\"confidence\",1.0) for x in row)/len(row)\n",
    "        lines.append({\"bbox\":[x0,y0,x1,y1], \"text\":text.strip(), \"source\":src, \"confidence\":conf})\n",
    "\n",
    "    # paragraph join\n",
    "    paras=[]\n",
    "    current=[lines[0]]\n",
    "    for ln in lines[1:]:\n",
    "        prev=current[-1]\n",
    "        if abs(ln[\"bbox\"][1]-prev[\"bbox\"][3]) <= para_gap:\n",
    "            current.append(ln)\n",
    "        else:\n",
    "            # flush\n",
    "            txt=\" \".join(x[\"text\"] for x in current if x[\"text\"])\n",
    "            x0=min(x[\"bbox\"][0] for x in current); y0=min(x[\"bbox\"][1] for x in current)\n",
    "            x1=max(x[\"bbox\"][2] for x in current); y1=max(x[\"bbox\"][3] for x in current)\n",
    "            src=\"+\".join(sorted(set(\",\".join(x[\"source\"] for x in current).split(\"+\"))))\n",
    "            conf=sum(x.get(\"confidence\",1.0) for x in current)/len(current)\n",
    "            paras.append({\"bbox\":[x0,y0,x1,y1], \"text\":txt.strip(), \"source\":src, \"confidence\":conf})\n",
    "            current=[ln]\n",
    "    if current:\n",
    "        txt=\" \".join(x[\"text\"] for x in current)\n",
    "        x0=min(x[\"bbox\"][0] for x in current); y0=min(x[\"bbox\"][1] for x in current)\n",
    "        x1=max(x[\"bbox\"][2] for x in current); y1=max(x[\"bbox\"][3] for x in current)\n",
    "        src=\"+\".join(sorted(set(\",\".join(x[\"source\"] for x in current).split(\"+\"))))\n",
    "        conf=sum(x.get(\"confidence\",1.0) for x in current)/len(current)\n",
    "        paras.append({\"bbox\":[x0,y0,x1,y1], \"text\":txt.strip(), \"source\":src, \"confidence\":conf})\n",
    "    return paras\n",
    "\n",
    "def deduplicate(blocks: List[Dict[str,Any]], iou_thr:float=0.45, sim_thr:float=0.90)->List[Dict[str,Any]]:\n",
    "    out=[]\n",
    "    for b in sorted(blocks, key=lambda x: (-x.get(\"confidence\",1.0), len(x.get(\"text\",\"\")), )):\n",
    "        t = (b.get(\"text\",\"\") or \"\").strip()\n",
    "        if not t: \n",
    "            continue\n",
    "        keep=True\n",
    "        for a in out:\n",
    "            if iou(b[\"bbox\"], a[\"bbox\"]) >= iou_thr:\n",
    "                if fuzz_ratio(t.lower(), a[\"text\"].lower())/100.0 >= sim_thr:\n",
    "                    keep=False; break\n",
    "        if keep: out.append(b)\n",
    "    return sorted(out, key=blocks_sort_key)\n",
    "\n",
    "def merge_ensemble(native: List[Dict], ocrs: List[List[Dict]], page_h: int) -> List[Dict]:\n",
    "    # union\n",
    "    all_blocks = []\n",
    "    all_blocks.extend(native)\n",
    "    for s in ocrs:\n",
    "        all_blocks.extend(s)\n",
    "    # regroup → dedup\n",
    "    regrouped = regroup_lines(all_blocks, line_join_px, para_join_px)\n",
    "    deduped   = deduplicate(regrouped, dedup_iou_thr, dedup_sim_thr)\n",
    "    # tag section\n",
    "    for b in deduped:\n",
    "        b[\"section\"] = section_of(b[\"bbox\"], page_h)\n",
    "    return deduped\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5) Main loop\n",
    "\n",
    "# %%\n",
    "if not fitz:\n",
    "    raise RuntimeError(\"PyMuPDF required for this step.\")\n",
    "\n",
    "doc  = fitz.open(pdf_path)\n",
    "meta = {\"pages\": len(doc), \"dpi\": dpi, \"langs\": langs, \"ensemble\":{\"easyocr\":use_easyocr,\"tesseract\":use_tesseract,\"trocr\":use_trocr}}\n",
    "(out_dir/\"metadata.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "for i in range(len(doc)):\n",
    "    page = doc[i]\n",
    "    # native\n",
    "    native = extract_native(page)\n",
    "    (out_dir/f\"page_{i+1:03d}_native.json\").write_text(json.dumps(native,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # rasterize + preprocess\n",
    "    pil = page_to_image(doc, i, dpi=dpi)\n",
    "    if mask_banners:\n",
    "        pil_for_ocr = mask_bands(pil, banner_top_pct, banner_bot_pct)\n",
    "    else:\n",
    "        pil_for_ocr = pil\n",
    "\n",
    "    pil_fast  = clamp_long_side(mask_bands(pil, banner_top_pct, banner_bot_pct) if mask_banners else pil, FAST_MAX_SIDE)\n",
    "    pil_heavy = clamp_long_side(pil, HEAVY_MAX_SIDE)\n",
    "\n",
    "    # OCR ensemble\n",
    "    o_easy = extract_easyocr(pil_for_ocr, langs) if use_easyocr else []\n",
    "    o_tess = extract_tesseract(pil_for_ocr, langs) if use_tesseract else []\n",
    "    o_troc = extract_trocr(pil_for_ocr) if use_trocr else []\n",
    "    o_padl = extract_paddleocr(pil_for_ocr, langs) if use_paddle else []   \n",
    "    o_sur  = extract_surya(pil_for_ocr) if use_surya else []\n",
    "\n",
    "    # keep raw outputs for traceability\n",
    "    (out_dir/f\"page_{i+1:03d}_ocr_easy.json\").write_text(json.dumps(o_easy,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    (out_dir/f\"page_{i+1:03d}_ocr_tess.json\").write_text(json.dumps(o_tess,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if use_trocr:\n",
    "        (out_dir/f\"page_{i+1:03d}_ocr_trocr.json\").write_text(json.dumps(o_troc,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if use_paddle:\n",
    "        (out_dir/f\"page_{i+1:03d}_ocr_paddle.json\").write_text(json.dumps(o_padl,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "    if use_surya:\n",
    "        (out_dir/f\"page_{i+1:03d}_ocr_surya.json\").write_text(json.dumps(o_sur,indent=2,ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # pick strategy if native weak\n",
    "    native_chars = sum(len(b.get(\"text\",\"\")) for b in native)\n",
    "    ocr_heads = [o_easy, o_tess, o_troc, o_padl, o_sur]\n",
    "    merged = merge_ensemble(native if native_chars >= native_len_gate else [],\n",
    "                            ocr_heads, pil.height)\n",
    "    # save merged\n",
    "    out_path = out_dir / f\"page_{i+1:03d}_blocks.json\"\n",
    "    out_path.write_text(json.dumps(merged, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    print(\n",
    "        f\"[page {i+1}] \"\n",
    "        f\"native={len(native)} \"\n",
    "        f\"easy={len(o_easy)} \"\n",
    "        f\"tess={len(o_tess)} \"\n",
    "        f\"trocr={len(o_troc)} \"\n",
    "        f\"paddle={len(o_padl)} \"\n",
    "        f\"surya={len(o_sur)} \"\n",
    "        f\"→ merged={len(merged)}\"\n",
    "    )\n",
    "\n",
    "    # optional viz overlay\n",
    "    if make_viz_png:\n",
    "        im = pil.copy()\n",
    "        dr = ImageDraw.Draw(im, \"RGBA\")\n",
    "        for b in merged:\n",
    "            x0,y0,x1,y1 = map(int, b[\"bbox\"])\n",
    "            src = (b.get(\"source\") or \"\").lower()\n",
    "            col = (122, 199, 136, 80)   # green-ish default\n",
    "            if \"easyocr\" in src: col=(255,165,0,90)   # orange\n",
    "            if \"tesseract\" in src: col=(70,130,180,90) # steelblue\n",
    "            if \"native\" in src: col=(147,112,219,90)   # purple\n",
    "            dr.rectangle([x0,y0,x1,y1], outline=(0,0,0,180), width=2, fill=col)\n",
    "        im.save(out_dir/f\"page_{i+1:03d}_viz.png\")\n",
    "\n",
    "print(\"✅ Done Extraction complete → :\", out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rxetl)",
   "language": "python",
   "name": "rxetl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
